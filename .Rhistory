dim(word.freq.pos)
dim(word.freq.neg)
dim(freq.all)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
freq.all[1:20,]
#Set NA's to 0
freq.all$freq.x[is.na(freq.all$freq.x)] = 0
freq.all$freq.y[is.na(freq.all$freq.y)] = 0
#Differences between Positive and Negative Frequencies
freq.all$diff = abs(freq.all$freq.x - freq.all$freq.y)
head(freq.all[order(-freq.all$diff), ])
#Smoothing term
alpha <- 2^7
#NDSI
freq.all$ndsi = abs(freq.all$freq.x -
freq.all$freq.y)/(freq.all$freq.x +
freq.all$freq.y +
2 * alpha)
#Sorting by NDSI
freq.all = freq.all[order(-freq.all$ndsi), ]
head(freq.all, 100)
#Convert word to a string
head(freq.all$word)
freq.all$word = as.character(freq.all$word)
head(freq.all$word)
# Term Frequencies and tfidf with NDSI ----
#AFINN Frequency Function (now used with ndsi lexicon)
freq.all$word = as.character(freq.all$word)
library(stringr)
ndsi.frequencies=function(x){
str_count(x,freq.all$word[1:1024]) #1024 words with highest NDSI score
}
length(freq.all$word)
ndsi.frequencies=function(x){
str_count(x,freq.all$word[1:1024]) #1024 words with highest NDSI score
}
term.freq <- t(apply(t(semisuper$clean), 2,
ndsi.frequencies))
inv.doc.freq=log(nrow(semisuper)/colSums(sign(term.freq)))
range(inv.doc.freq)
inv.doc.freq[is.infinite(inv.doc.freq)]=0
range(inv.doc.freq)
tf.idf = term.freq %*% diag(inv.doc.freq)
semisuper$polarity=as.factor(semisuper$polarity)
rf.semisuper=data.frame(polarity=semisuper$polarity,tf.idf)
library(rpart)
tree = rpart(polarity~., data = rf.semisuper)
pred.sentiment=predict(tree,
newdata=rf.movie.data[-train,])
tree
predict(tree, newdata = semisuper, type = "prob")
predict(tree, newdata = semisuper)
help(predict)
predict(tree, newdata = rf.semisuper)
predict(tree, newdata = rf.semisuper, type = "class")
table(predict(tree, newdata = rf.semisuper, type = "class"))
tree
library(ctree)
print("hello world")
library(stringr) #library for str_count function
library(e1071) # for naive bayes model
library(ggplot2) #for graphs
library(caret) #for confusionMatrix
library(pROC) #ROC curves
library(randomForest)
library(tm) # for building term frequency matrix from corpus
source("functions.R") #get cleaning function, AFINN_lexicon
happy = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", nrows = 110000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
sad = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", nrows = 50000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
#Clean tweets, remove blank tweets, then undersample from happy so that happy has as many tweets as sad does.
happy$text = clean.data(happy$text)
sad$text = clean.data(sad$text)
happy  = happy[happy$text!="",]
sad  = sad[sad$text!="",]
set.seed(127)
index = sample(nrow(happy),nrow(sad))
happy = happy[index,]
dim(happy)
dim(sad) #both should have 48124 rows
happy$polarity = 1
sad$polarity = -1
semisuper = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
colnames(semisuper) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(semisuper)
table(semisuper$polarity)
val = read.csv("testdata.manual.2009.06.14.csv")
dim(val)
head(val)
head(val,2)
rm(val)
val[10:15]
test[10:15,]
test = read.csv("testdata.manual.2009.06.14.csv")
test[10:15,]
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE)
head(test)
test
colnames(test) = c("polarity", "not_sure", "time", "search_query", "username", "text")
head(test)
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("integer", "integer", "character", "character", "character", "string"))
c("numeric", "integer", "character", "character", "character", "string"))
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("numeric", "integer", "character", "character", "character", "string"))
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("text", "text", "character", "character", "character", "string"))
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "time", "search_query", "username", "text")
test$polarity = as.factor(test$polarity)
test$time = as.POSIXct(test$time)
test$time = as.POSIXlt.factor(test$time)
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test$polarity = as.factor(test$polarity)
test$polarity = as.factor(test$polarity)
happy$polarity = 1
sad$polarity = 0
semisuper = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
colnames(semisuper) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(semisuper)
table(semisuper$polarity)
class(semisuper$polarity)
happy = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", nrows = 110000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
sad = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", nrows = 50000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
#Clean tweets, remove blank tweets, then undersample from happy so that happy has as many tweets as sad does.
happy$text = clean.data(happy$text)
sad$text = clean.data(sad$text)
happy  = happy[happy$text!="",]
sad  = sad[sad$text!="",]
happy$polarity = as.factor(1)
sad$polarity = as.factor(0)
train = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
class(train$polarity)
train = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
colnames(train) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(train)
table(train$polarity)
set.seed(127)
index = sample(nrow(happy),nrow(sad))
happy = happy[index,]
dim(happy)
dim(sad) #both should have 48124 rows
happy$polarity = as.factor(1)
sad$polarity = as.factor(0)
train = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
colnames(train) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(train)
table(train$polarity)
test[test$polarity == 4, ]$polarity = as.factor(1)
test[test$polarity == 4,]
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test[1:10,]
test$polarity == 4
test[test$polarity == 4,]
test[test$polarity == 4,]$polarity
test[test$polarity == 4,]$polarity = as.factor(1)
test[test$polarity == 2,]$polarity = as.factor(NA)
test[test$polarity == 0,]$polarity = as.factor(0)
test[test$polarity == 0,]$polarity = as.factor(0)
test$polarity == 0
happy$polarity = as.factor(1)
sad$polarity = as.factor(0)
train = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
colnames(train) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(train)
table(train$polarity)
# validation data: sentiment140
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test[test$polarity == 4,]$polarity = as.factor(1)
test[test$polarity == 0,]$polarity = as.factor(0)
test$polarity = as.factor(test$polarity)
head(test)
head(test, 200)
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
head(test, 200)
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test[test$polarity == 0,]$polarity = as.factor(0)
test[test$polarity == 4,]$polarity = as.factor(1)
table(test$polarity)
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
table(test$polarity)
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
table(test$polarity)
test[test$polarity == 0,]$polarity = 0
test[test$polarity == 4,]$polarity = 1
table(test$polarity)
table(test[test$polarity !=2,])
table(test[test$polarity !=2,]$polarity)
test = test[test$polarity !=2, c("polarity", "text")]
test
x = read.csv(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.csv", nrows = 9400000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
length(grep("=\\)", x$text, value = TRUE)) # =)'s in the whole set
length(grep("\\(=", x$text, value = TRUE)) # (='s in the whole set
grep("\\(=", x$text, value = TRUE) #195 (='s in the whole set
grep("=\\)", x$text, value = TRUE) #1467 =)'s in the whole set
grep("â˜º", x$text, value = TRUE) #1467 =)'s in the whole set
length(grep("=\\(", x$text, value = TRUE)) # =('s in the whole set
length(grep("\\)=", x$text, value = TRUE)) # )='s in the whole set
length(grep("\\)=", x$text, value = TRUE)) # )='s in the whole set
grep("=\\(", x$text, value = TRUE) #59 =('s in the whole set
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test[test$polarity == 0,]$polarity = 0
test[test$polarity == 4,]$polarity = 1
table(test$polarity)
test = test[test$polarity !=2, c("polarity", "text")]
test$polarity = as.factor(test$polarity)
head(test, 200)
test$clean = clean.data(test$text)
a = Sys.time()
term.freq <- t(apply(t(test$clean), 2,
AFINN_lexicon.frequencies))
Sys.time()-a
dim(term.freq)
semisuper$AFINN.rating = as.vector(term.freq %*% AFINN_lexicon$score)
test$AFINN.rating = as.vector(term.freq %*% AFINN_lexicon$score)
test
test[c("text","AFINN.rating")]
test[c("text","AFINN.rating", "polarity")]
test$AFINN.rating = as.vector(term.freq %*% AFINN_lexicon$score)
test$pred = sign(test$AFINN.rating)
table(test$pred)
table(test$pred, test$polarity)
table(test$polarity, test$pred)
(102+142)/(102+17+50+142)
length(grep("\\]:", x$text, value = TRUE)) #70 ]:'s in the whole set.
length(grep(":\\{", x$text, value = TRUE)) #32 }:'s in the whole set.
length(grep(":\\{", x$text, value = TRUE)) #32 :{'s in the whole set.
length(grep(":\\{", x$text, value = TRUE)) #32 }:'s in the whole set.
length(grep("\\}:", x$text, value = TRUE)) #32 }:'s in the whole set.
word.freq <- function(document.vector, sparsity = .99){
# construct corpus
temp.corpus <- Corpus(VectorSource(document.vector))
# construct tf matrix and remove sparse terms
temp.tf <- DocumentTermMatrix(temp.corpus,
control = list(stopwords = stopwords('english'),
removeNumbers = T))
temp.tf <- removeSparseTerms(temp.tf, sparsity)
temp.tf <- as.matrix(temp.tf)
# construct word frequency df
freq.df <- colSums(temp.tf)
freq.df <- data.frame(word = names(freq.df), freq = freq.df)
rownames(freq.df) <- NULL
return(freq.df)
}
fruit.names=c("apple","banana","cantaloupe","date")
str_count("i want to eat an apple or a banana, preferably an apple",fruit.names)
str_count("i want to eat an apples or a banana, preferably an apple",fruit.names)
str_count("i want to eat an applesauce or a banana, preferably an apple",fruit.names)
str_count("i want to eat an applesauces or a banana, preferably an apple",fruit.names)
fruit.names=c("app","banana","cantaloupe","date")
str_count("i want to eat an applesauces or a banana, preferably an apple",fruit.names)
fruit.names=c("happy","banana","cantaloupe","date")
str_count("i want to eat an #ohhappyday or a banana, preferably an apple",fruit.names)
rm(x)
write.csv(Wiebe_lexicon, file = "Wiebe_lexicon.csv")
Wiebe_lexicon = read.csv(system.file("data/subjectivity.csv.gz",
package = "sentiment"), header = FALSE, stringsAsFactors = FALSE)
write.csv(Wiebe_lexicon, file = "Wiebe_lexicon.csv")
Wiebe_lexicon = read.csv(system.file("data/subjectivity.csv.gz",
package = "sentiment"), header = FALSE, stringsAsFactors = FALSE)
write.csv(Wiebe_lexicon, file = "Wiebe_lexicon.csv")
source("functions.R") #get cleaning function, AFINN_lexicon
library(stringr) #library for str_count function
library(e1071) # for naive bayes model
library(ggplot2) #for graphs
library(caret) #for confusionMatrix
library(pROC) #ROC curves
library(randomForest)
library(tm) # for building term frequency matrix from corpus
(102+142)/(102+17+50+142)
(26640+16671)/(26640+7644+16671+16291)
(119+124)/(119+124+39+33)
precision = 124/(124+39) # 76%
recall =  124/(124+25+33) # 68%
f1 = (2*precision*recall)/(precision + recall) #71.8%
f1
(22691+21023)/(22691+21023+9783+11464)
precision = 22957/(22957+11957) # 65%
recall =  22957/(22957+15912+9870) # 47%
f1 = (2*precision*recall)/(precision + recall) #54.8%
f1
(102+142)/(102+142+17+50)
precision = 142/(142+50) # 73.9%
recall =  142/(142+23+17) # 78.0%
f1 = (2*precision*recall)/(precision + recall) # 75.9%
f1
22957/(22957+11957)
(117+78)/(117+78+22+40)
precision = 117/(117+40) # 74%
recall =  117/(117+43+22) # 64%
f1 = (2*precision*recall)/(precision + recall) # 69%
f1
(20985+14471)/(20985+14471+7081+12928)
precision = 20985/(20985+12928) # 61%
recall =  20985/(20985+7081+20673) # 43%
f1 = (2*precision*recall)/(precision + recall) # 50%
f1
(73+86)/(73+86+45+56)
precision = 86/(86+56) # 60.5%
recall =  86/(86+51+45) # 47.2%
f1 = (2*precision*recall)/(precision + recall) # 53.1%
f1
(17090+16398)/(17090+16398+10326+9463)
precision = 17090/(17090+9463) # 64.3%
recall =  17090/(17090+21323+10326) # 35%
f1 = (2*precision*recall)/(precision + recall) # 45.4%
f1
(153+104)/(153+104+74+20)
precision = 104/(104+20) # 84%
recall =  104/(104+4+74) # 57%
f1 = (2*precision*recall)/(precision + recall) # 68%
f1
20830/(20830+11532)
20830/(20830+8388+19521)
(28173+20830)/(28173+20830+19521+11532)
precision = 20830/(20830+11532) # 64%
recall =  20830/(20830+8388+19521) # 42%
f1 = (2*precision*recall)/(precision + recall) # 51%
f1
rm(x)
rm(tree)
rm(fruit.names)
rm(alpha)
rm(a)
rm(rf.semisuper)
rm(term.freq)
rm(tf.idf)
rm(train)
rm(test)
rm(semisuper)
rm(freq.all)
rm(word.freq.neg)
rm(word.freq.pos)
happy_tweets = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", nrows = 150000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
dim(happy_tweets)
sad_tweets = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", nrows = 55000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
dim(sad_tweets)
happy_tweets$clean = clean.data(happy_tweets$text)
sad_tweets$clean = clean.data(sad_tweets$text)
happy_corpus = Corpus(VectorSource(happy_tweets$clean))
sad_corpus = Corpus(VectorSource(sad_tweets$clean))
happy_corpus <- tm_map(happy_corpus, function(x)removeWords(x,stopwords())) # ~2 minutes
sad_corpus <- tm_map(sad_corpus, function(x)removeWords(x,stopwords())) # ~1 minute
#Make word cloud. Check reference for options
wordcloud(happy_corpus, max.words = 200) # ~4 minutes
wordcloud(sad_corpus, max.words = 200) # ~3 minutes
library(wordcloud)
library(tm)
x = read.csv(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.csv", nrows = 940, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
head(x)
wordcloud(happy_corpus, max.words = 200) # ~4 minutes
wordcloud(sad_corpus, max.words = 200) # ~3 minutes
wordcloud(happy_corpus, max.words = 200) # ~4 minutes
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test[test$polarity == 0,]$polarity = 0
test[test$polarity == 4,]$polarity = 1
table(test$polarity)
dim(test)
head(test)
head(test[c(text, polarity)])
head(test[c("text", "polarity"),])
head(test)
test[c("text", "polarity"),]
test
head(test)
table(test$polarity)
head(test,20)
load("my_oauth.Rdata")
happy = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", nrows = 110000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
sad = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", nrows = 50000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
head(happy)
head(happy, 25)
head(happy$text, 25)
head(happy$text, 500)
head(happy$text, 1000)
head(happy$text, 10000)
head(sad$text, 10000)
blurbs = c("hello", "world", "How are you?")
grep("hello", blurbs)
grep("world", blurbs)
grep(c("hello","world"), blurbs)
grep(hello|world", blurbs)
)
grep("hello|world", blurbs)
x = read.csv(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.csv", nrows = 100000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
grep("\\:\\)", x$text, value = TRUE) #67189 :)'s in the whole set. Takes about 11 sec. to run
grep( "\\:\\) | \\(\\: | \\:-\\) | \\(-\\:", x$text, value = TRUE)
grep("\\:\\)", x$text, value = TRUE) #67189 :)'s in the whole set. Takes about 11 sec. to run
grep("\\(\\:", x$text, value = TRUE) #14401 (:'s in the whole set.
grep( "\\:\\)|\\(\\:|\\:-\\)|\\(-\\:", x$text, value = TRUE)
grep( "\\:\\)|\\(\\:|\\:-\\)|\\(-\\:|\\:D|\\:-D|=\\)|\\(=", x$text, value = TRUE)
grep( "\\:\\)|\\(\\:|\\:-\\)|\\(-\\:|\\:D|\\:-D|=\\)|\\(=|â˜º|â˜»", x$text, value = TRUE)
x = read.csv(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.csv", nrows = 9400000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
length(grep("\\:\\)|\\(\\:|\\:-\\)|\\(-\\:|\\:D|\\:-D|=\\)|\\(=|â˜º|â˜»", x$text, value = FALSE))
happy_indices = c(
grep("\\:\\)", x$text, value = FALSE),
grep("\\(\\:", x$text, value = FALSE),
grep("\\:-\\)", x$text, value = FALSE),
grep("\\(-\\:", x$text, value = FALSE),
grep("\\:D", x$text, value = FALSE),
grep("\\:-D", x$text, value = FALSE),
grep("=\\)", x$text, value = FALSE),
grep("\\(=", x$text, value = FALSE),
grep("â˜º", x$text, value = FALSE),
grep("â˜»", x$text, value = TRUE)
)
length(happy_indices)
length(happy_indices) #happy_indices has length 148000
dim(x[happy_indices[duplicated(happy_indices)],]) #698 entries have multiple distinct happy emoticons
dim(unique(x[happy_indices,])) #There are 147176 unique happy rows
happy_indices2 = length(grep("\\:\\)|\\(\\:|\\:-\\)|\\(-\\:|\\:D|\\:-D|=\\)|\\(=|â˜º|â˜»", x$text, value = FALSE))
happy_indices2
happy_indices2 = grep("\\:\\)|\\(\\:|\\:-\\)|\\(-\\:|\\:D|\\:-D|=\\)|\\(=|â˜º|â˜»", x$text, value = FALSE)
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "â˜º", "â˜»")
paste(happy_emoticons, sep = "|")
help("paste")
happy_emoticons = list("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "â˜º", "â˜»")
paste(happy_emoticons, sep = "|")
pate(1:10)
paste(1:10)
1:10
paste(1:10, sep = "")
paste(1:10, sep = "   ")
paste(1:10, sep = "|", collapse = "")
paste(happy_emoticons, sep = "|", collapse = "")
paste(happy_emoticons, collapse = "|")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "â˜¹")
paste(sad_emoticons, collapse = "|")
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "â˜º", "â˜»")
length(happy_indices2)
sad_indices2 = grep(paste(sad_emoticons, collapse = "|"),,x$text, value = FALSE)
sad_indices2 = grep(paste(sad_emoticons, collapse = "|"),x$text, value = FALSE)
length(ssad_indices2)
length(sad_indices2)
52498-52247
head(sad_indices)
head(happy_indices)
head(happy_indices2)
help(match)
happy_indices %in% happy_indices2
table(happy_indices %in% happy_indices2)
table(happy_indices2 %in% happy_indices)
table(happy_indices2 %in% happy_indices, happy_indices %in% happy_indices2)
index(happy_indices2 %in% happy_indices)
x[happy_indices2 %in% happy_indices,]
help(load)
x = load(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.Rdata")
x = load(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.RData")
# HAPPY EMOTICONS ----
x = load(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.RData")
load(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.RData")
load("~/Desktop/Huang Research/Rsentiment/ComTweetsLA.RData")
load("~/Desktop/Huang Research/Rsentiment/comTweetsLA.RData")
AFINN = read.delim(file = "~/Desktop/Documents/GitRepos/Rsentiment/Lexicons/AFINN/AFINN-111.txt", stringsAsFactors = FALSE)
colnames(AFINN) = c("word", "score")
#looking up AFINN words
AFINN[AFINN$score == -5,]
AFINN[pmatch("am", AFINN$word),2]
AFINN[pmatch("feeling", AFINN$word),2]
AFINN[pmatch("creaking", AFINN$word),2]
AFINN[pmatch("plague", AFINN$word),2]
OpinionFinder = read.csv(system.file("data/subjectivity.csv.gz",
package = "sentiment"), header = FALSE, stringsAsFactors = FALSE)
#write.csv(OpinionFinder, file = "OpinionFinder")
OpinionFinder = as.data.frame(cbind(as.character(OpinionFinder$V1), as.integer(2*(OpinionFinder$V3 == "positive")-1)))
colnames(OpinionFinder) = c("word", "score")
OpinionFinder$score = as.integer(OpinionFinder$score)
OpinionFinder$score = (((OpinionFinder$score-1)*2)-1)*-1
#looking up Wiebe words
OpinionFinder[OpinionFinder$score == -1,]
OpinionFinder[pmatch("asu", OpinionFinder$word),] #the word "asu" matches "asunder" and gets a score of -1!!!
OpinionFinder[pmatch("you", OpinionFinder$word),]
NRC = read.csv(file = "~/Desktop/Documents/GitRepos/Rsentiment/Lexicons/EmoLex/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt",
sep = "\t", header = FALSE)
colnames(NRC) = c("word", "emotion", "indicator")
NRC = NRC[NRC$emotion == "negative"|NRC$emotion == "positive",]
NRC = NRC[NRC$indicator == 1,]
NRC[NRC$emotion == "negative",]$indicator = -1
NRC = NRC[c("word", "indicator")]
colnames(NRC) = c("word", "score")
ANEW = read.csv(file = "~/Desktop/Documents/GitRepos/Rsentiment/Lexicons/ANEW.csv", header = FALSE)
colnames(ANEW) = c("word", "score")
#ANEW$score = ANEW$score-(mean(ANEW$score)+1) #normalize ANEW scores to 0. This didn't work as well as the next line did.
ANEW$score = ANEW$score - 5
range(ANEW$score)
head(ANEW[order(-ANEW$score),], 20) #happiest words
head(ANEW[order(ANEW$score),], 20) #saddest words
