tf.idf[,106]
tf.idf[,1000]
tf.idf[,1001]
tf.idf[,1002]
tf.idf[,1003]
tf.idf[,1004]
pred.sentiment=predict(rf.model, newdata = tf.idf)
load(file = paste(storage.directory, "rf.model.RData", sep = ""))
pred.sentiment=predict(rf.model, newdata = tf.idf)
colnames(tf.idf)
head(tf.idf)
class(tf.idf)
as.matrix(tf.idf)
as.df(tf.idf)
as.data.frame(tf.idf)
pred.sentiment=predict(rf.model, newdata = as.data.frame(tf.idf))
dim(tf.idf)
tf.idf = as.data.frame(tf.idf)
colnames(tf.idf)
paste("X", 1:1024)
paste("X", 1:1024, sep = "")
colnames(tf.idf) = paste("X", 1:1024, sep = "")
pred.sentiment=predict(rf.model, newdata = as.data.frame(tf.idf))
pred.sentiment
test$polarity
confusionMatrix(pred.sentiment,test$polarity)
phat=predict(rf.model,
newdata = tf.idf,
type = "prob")
plot(roc(emoticon.tf.idf$polarity,phat[,2]))
plot(roc(tf.idf$polarity,phat[,2]))
plot(roc(test$polarity,phat[,2]))
colSums(sign(term.freq))
ndsi.frequencies
ndsi.frequencies$word
ndsi.lexicon
rm(freq.all)
term.freq <- t(apply(t(test[,"clean"]), 2,    #TAKES TIME; 10 minute for 100 000 tweets and 1276 terms
ndsi.frequencies))
term.freq <- t(apply(t(test[,"clean"]), 2,    #TAKES TIME; 10 minute for 100 000 tweets and 1276 terms
ndsi.frequencies))
load(paste(storage.directory,"freq.all.RData", sep = "")) # load freq.all lexicon into memory as freq.all
ndsi.lexicon = freq.all[freq.all$ndsi>0.05,]
dim(ndsi.lexicon)
# Apply freq.all to test data (2 secs for test's 359 rows, but otherwise costly)
term.freq <- t(apply(t(test[,"clean"]), 2,    #TAKES TIME; 10 minute for 100 000 tweets and 1276 terms
ndsi.frequencies))
tf.idf = as.data.frame(tf.idf)
colnames(tf.idf) = paste("X", 1:1024, sep = "") #hacky fix for column names
pred.sentiment=predict(rf.model, newdata = as.data.frame(tf.idf))
#Accuracy
confusionMatrix(pred.sentiment,test$polarity) # Accuracy is a respectable 68%
# ROC curve
phat=predict(rf.model,
newdata = tf.idf,
type = "prob")
plot(roc(test$polarity,phat[,2])) #Even more respectable 76.65%
head(tf.idf)
rm(tf.idf)
tf.idf = as.data.frame(term.freq)
colnames(tf.idf) = paste("X", 1:1024, sep = "") #hacky fix for column names
pred.sentiment=predict(rf.model, newdata = as.data.frame(tf.idf))
#Accuracy
confusionMatrix(pred.sentiment,test$polarity) # Accuracy is a respectable 68%
phat=predict(rf.model,
newdata = tf.idf,
type = "prob")
plot(roc(test$polarity,phat[,2])) #Even more respectable 76.65%
term.freq <- t(apply(t(test[,"clean"]), 2,    #TAKES TIME; 10 minute for 100 000 tweets and 1276 terms
ndsi.frequencies))
# More magic here. Need to understand this better
inv.doc.freq=log(nrow(emoticon)/colSums(sign(term.freq)))
range(inv.doc.freq)
inv.doc.freq[is.infinite(inv.doc.freq)]=0
range(inv.doc.freq)
tf.idf = term.freq %*% diag(inv.doc.freq)
# Prediction
tf.idf = as.data.frame(tf.idf)
colnames(tf.idf) = paste("X", 1:1024, sep = "") #hacky fix for column names
pred.sentiment=predict(rf.model, newdata = as.data.frame(tf.idf))
#Accuracy
confusionMatrix(pred.sentiment,test$polarity) # Accuracy is a respectable 68%
# ROC curve
phat=predict(rf.model,
newdata = tf.idf,
type = "prob")
plot(roc(test$polarity,phat[,2])) #Even more respectable 76.65%
range(inv.doc.freq)
tf.idf = term.freq %*% diag(inv.doc.freq)
Sys.time()-a
rm(freq.all)
a = Sys.time()
term.freq <- t(apply(t(emoticon[,"clean"]), 2,    #TAKES TIME; 10 minutes for 100 000 tweets and 1276 terms
ndsi.frequencies))
Sys.time()-a
freq.all = freq.all[freq.all$ndsi>0.05,]
load(paste(storage.directory,"freq.all.RData", sep = "")) # load freq.all lexicon into memory as freq.all
freq.all = freq.all[freq.all$ndsi>0.05,]
dim(freq.all)
a = Sys.time()
term.freq <- t(apply(t(emoticon[,"clean"]), 2,    #TAKES TIME; 10 minutes for 100 000 tweets and 1276 terms
ndsi.frequencies))
Sys.time()-a
tf.idf = term.freq %*% diag(inv.doc.freq)
save(inv.doc.freq, file = paste(storage.directory, "inv.doc.freq.RData", sep = "")) #save inv.doc freq for classifier
diag(inv.doc.freq)
help(d9ag)
help(diag)
tf.idf = term.freq %*% diag(inv.doc.freq)
dim(term.freq)
term.freq <- t(apply(t(test[,"clean"]), 2,    #TAKES TIME; 10 minute for 100 000 tweets and 1276 terms
ndsi.frequencies))
tf.idf = term.freq %*% diag(inv.doc.freq)
tf.idf = as.data.frame(tf.idf)
colnames(tf.idf) = paste("X", 1:1024, sep = "") #hacky fix for column names
pred.sentiment=predict(rf.model, newdata = as.data.frame(tf.idf))
#Accuracy
confusionMatrix(pred.sentiment,test$polarity) # Accuracy is a respectable 68%
phat=predict(rf.model,
newdata = tf.idf,
type = "prob")
plot(roc(test$polarity,phat[,2])) #Even more respectable 76.65%
load(file = paste(storage.directory, "inv.doc.freq.RData", sep = ""))
help("%*%")
diag(inv.doc.freq)
rm(rf.model)
rm(pred.sentiment)
rm(a)
rm(alpha)
rm(inv.doc.freq)
rm(phat)
rm(term.freq)
rm(freq.all)
rm(emoticon)
rm(ndsi.lexicon)
rm(test)
rm(tf.idf)
source("functions.R")
load("~/Desktop/Huang Research/Rsentiment/emoticon.RData") # load train/emoticon into memory as emoticon
table(emoticon$polarity)
dim(emoticon)
word.freq.pos = word.freq(emoticon$clean[emoticon$polarity == 1],
sparsity=0.9999) #terms must occur in at least 1 out of 1000 tweets
word.freq.neg = word.freq(emoticon$clean[emoticon$polarity == 0],
sparsity=0.9999)
dim(word.freq.pos)
dim(word.freq.neg)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
#Merge by word
freq.all = merge(word.freq.neg, word.freq.pos, by = 'word', all = T)
dim(word.freq.pos)
dim(word.freq.neg)
dim(freq.all)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
freq.all[1:20,]
#Set NA's to 0
freq.all$freq.x[is.na(freq.all$freq.x)] = 0
freq.all$freq.y[is.na(freq.all$freq.y)] = 0
#Differences between Positive and Negative Frequencies
freq.all$diff = abs(freq.all$freq.x - freq.all$freq.y)
head(freq.all[order(-freq.all$diff), ],100) #this is somewhat puzzling
#Smoothing term
alpha <- 2^7
#NDSI
freq.all$ndsi = abs(freq.all$freq.x -
freq.all$freq.y)/(freq.all$freq.x +
freq.all$freq.y +
2 * alpha)
#Sorting by NDSI
freq.all = freq.all[order(-freq.all$ndsi), ]
head(freq.all, 100)
#Convert word to a string
head(freq.all$word)
freq.all$word = as.character(freq.all$word)
head(freq.all$word,100)
freq.all = freq.all[freq.all$ndsi>0,] # restrict to words with a nonzero ndsi score.
freq.all$word = as.character(freq.all$word)
dim(freq.all)
head(freq.all)
head(freq.all,25)
head(freq.all)
help(remove)
freq.all = freq.all[freq.all$word != "sadtoken" & freq.all$word != "happytoken",]
head(freq.all)
save(freq.all, file = paste(storage.directory,"freq.all.RData", sep = "")) # save freq.all into memory as freq.all.RData
freq.all = freq.all[freq.all$ndsi>0.05,]
dim(freq.all)
tail(freq.all)
rm(wword.freq.neg)
rm(word.freq.pos)
rm(word.freq.neg)
rm(freq.all)
a = Sys.time()
term.freq <- t(apply(t(emoticon[,"clean"]), 2,    #TAKES TIME; 10 minutes for 100 000 tweets and 1276 terms
ndsi.frequencies))
Sys.time()-a
load(paste(storage.directory,"freq.all.RData", sep = "")) # load freq.all lexicon into memory as freq.all
freq.all = freq.all[freq.all$ndsi>0.05,]
dim(freq.all)
a = Sys.time()
term.freq <- t(apply(t(emoticon[,"clean"]), 2,    #TAKES TIME; 10 minutes for 100 000 tweets and 1276 terms
ndsi.frequencies))
Sys.time()-a
inv.doc.freq=log(nrow(emoticon)/colSums(sign(term.freq)))
range(inv.doc.freq)
inv.doc.freq[is.infinite(inv.doc.freq)]=0
range(inv.doc.freq)
#SAVE inv.doc.freq for later use. In particular we care about its diagonal.
save(inv.doc.freq, file = paste(storage.directory, "inv.doc.freq.RData", sep = "")) #save inv.doc freq for classifier
tf.idf = term.freq %*% diag(inv.doc.freq)
save(tf.idf, file = paste(storage.directory,"tf.idf.RData", sep = "")) # save tf.idf lexicon into memory as tf.idf
emoticon.tf.idf = data.frame(polarity=emoticon$polarity, tf.idf)
save(emoticon.tf.idf, file = paste(storage.directory,"emoticon.tf.idf.RData", sep = "")) # save emoticon.tf.idf lexicon into memory as tf.idf
load("~/Desktop/Huang Research/Rsentiment/emoticon.RData") # load train/emoticon into memory as emoticon
dim(emoticon)
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test[test$polarity == 0,]$polarity = 0
test[test$polarity == 4,]$polarity = 1
table(test$polarity)
test = test[test$polarity !=2, c("polarity", "text")]
test$polarity = as.factor(test$polarity)
test$clean = clean.tweets(test$text)
term.freq.test <- t(apply(t(test$clean), 2, AFINN_lexicon.frequencies))
test$AFINN.rating = as.vector(term.freq.test %*% AFINN_lexicon$score)
test$pred = sign(test$AFINN.rating)
table(test$pred)
table(test$polarity, test$pred)
test$AFINN.rating2 = classify.sentiment(test$clean)
test$AFINN.rating2.pred = sign(test$AFINN.rating2)
table(test$AFINN.rating2.pred)
table(test$polarity, test$AFINN.rating2.pred)
AFINN_lexicon
table(test$polarity, test$AFINN.rating2.pred)
ans = table(test$polarity, test$AFINN.rating2.pred)
#accuracy is (ans[1,1]+ans[2,3])/(sum(ans[,1])+sum(ans[,3])) = 77.14% (excluding neutral tweets)
(ans[1,1]+ans[2,3])/(sum(ans[,1])+sum(ans[,3])) # accuracy is 77.14% (excluding neutral tweets)
(120+129)/(120+129+31+39)
ans[1,1]/sum(ans[,3]) # precision is
ans[2,3]/sum(ans[,3]) # precision is
129/(129+39)
ans[2,3]/sum(ans[2,]) # precision is 76.78%
129/(129+22+31)
(ans[1,1]+ans[2,3])/(sum(ans[,1])+sum(ans[,3])) # accuracy is 78.05% (excluding neutral tweets)
accuracy = (ans[1,1]+ans[2,3])/(sum(ans[,1])+sum(ans[,3]))
accuracy #is 78.05% (excluding neutral tweets)
precision = ans[2,3]/sum(ans[,3]) # precision is 76.78%
precision = ans[2,3]/sum(ans[,3])
precision # precision is 76.78%
recall = ans[2,3]/sum(ans[2,])
recall # recall is 70.87%
f1 = (2*precision*recall)/(precision + recall) #71.88%
f1 #is
train$AFINN.rating2 = classify.sentiment(train$clean)
load(file = paste(storage.directory, "sent140.RData", sep = ""))
dim(test)
sent140 = test
save(sent140, file = paste(storage.directory, "sent140.RData", sep = ""))
load(file = paste(storage.directory, "sent140.RData", sep = ""))
rm(ssent140)
rm(test)
rm(sent140)
load(file = paste(storage.directory, "sent140.RData", sep = ""))
dim(sent140)
sent140$AFINN.rating3 = classify.sentiment2(sent140$clean)
emoticon$AFINN.rating = classify.sentiment(emoticon$clean)
dim(emoticon)
emoticon$AFINN.rating.pred = sign(emoticon$AFINN.rating)
table(emoticon$AFINN.rating.pred)
ans = table(emoticon$polarity, emoticon$AFINN.rating.pred)
ans
accuracy = (ans[1,1]+ans[2,3])/(sum(ans[,1])+sum(ans[,3]))
accuracy #is 78.05% (excluding neutral tweets)
head(emoticon)
tail(emoticon)
table(emoticon$polarity)
table(emoticon$AFINN.rating.pred)
ans
ans$-1
ans["1",]
ans["1","1"]
ans["1","-1"]
ans = table(sent140$polarity, sent140$AFINN.rating.pred)
sent140$AFINN.rating = classify.sentiment(sent140$clean)
sent140$AFINN.rating.pred = sign(sent140$AFINN.rating)
table(sent140$AFINN.rating.pred)
ans = table(sent140$polarity, sent140$AFINN.rating.pred)
ans
accuracy = (ans["0","-1"]+ans["1","1"])/(sum(ans[,"-1"])+sum(ans[,"1"]))
accuracy #is 78.05% (excluding neutral tweets)
precision = ans["1","1"]/sum(ans[,"1"])
precision # precision is 76.78%
recall = ans["1","1"]/sum(ans["1",])
recall # recall is 70.87%
f1 = (2*precision*recall)/(precision + recall)
f1 #is 73.71%
emoticon$AFINN.rating.pred = sign(emoticon$AFINN.rating)
table(emoticon$AFINN.rating.pred)
ans = table(emoticon$polarity, emoticon$AFINN.rating.pred)
ans
accuracy = (ans["0","-1"]+ans["1","1"])/(sum(ans[,"-1"])+sum(ans[,"1"]))
accuracy #is 78.05% (excluding neutral tweets)
dim(AFINN_lexicon)
dim(AFINN_lexicon[AFINN_lexicon$word != "sadtoken" & AFINN_lexicon$word != "happytoken"])
dim(AFINN_lexicon[AFINN_lexicon$word != "sadtoken" & AFINN_lexicon$word != "happytoken",])
dim(AFINN_lexicon[AFINN_lexicon$word != "sadtoken" & AFINN_lexicon$word != "happytoken",])
AFINN_lexicon = AFINN_lexicon[AFINN_lexicon$word != "sadtoken" & AFINN_lexicon$word != "happytoken",]
dim(AFINN_lexicon)
emoticon$AFINN.rating = classify.sentiment(emoticon$clean)
emoticon$AFINN.rating.pred = sign(emoticon$AFINN.rating)
table(emoticon$AFINN.rating.pred)
ans = table(emoticon$polarity, emoticon$AFINN.rating.pred)
ans
accuracy = (ans["0","-1"]+ans["1","1"])/(sum(ans[,"-1"])+sum(ans[,"1"]))
accuracy #is 78.05% (excluding neutral tweets)
precision = ans["1","1"]/sum(ans[,"1"])
precision # precision is 76.78%
recall = ans["1","1"]/sum(ans["1",])
recall # recall is 70.87%
f1 = (2*precision*recall)/(precision + recall)
f1 #is 73.71%
sent140$WIEBE.rating = classify.sentiment(sent140$clean, lexicon = Wiebe_lexicon)
#List of negation words
negations = c("no", "not","none","nobody","nothing","neither","never","doesnt","isnt","wasnt","shouldnt","wouldnt", "couldnt","wont","cant","dont")
ANEW = read.csv(file = "~/Desktop/Documents/GitRepos/Rsentiment/Lexicons/ANEW.csv", header = FALSE)
colnames(ANEW) = c("word", "score")
#ANEW$score = ANEW$score-(mean(ANEW$score)+1) #normalize ANEW scores to 0. This didn't work as well as the next line did.
ANEW$score = ANEW$score - 5
range(ANEW$score)
head(ANEW[order(-ANEW$score),], 20) #happiest words
head(ANEW[order(ANEW$score),], 20) #saddest words
#NRC Word-Emotion Association Lexicon
NRC = read.csv(file = "~/Desktop/Documents/GitRepos/Rsentiment/Lexicons/EmoLex/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt",
sep = "\t", header = FALSE)
colnames(NRC) = c("word", "emotion", "indicator")
NRC = NRC[NRC$emotion == "negative"|NRC$emotion == "positive",]
NRC = NRC[NRC$indicator == 1,]
NRC[NRC$emotion == "negative",]$indicator = -1
NRC = NRC[c("word", "indicator")]
colnames(NRC) = c("word", "score")
#OpinionFinder Lexicon
OpinionFinder = read.csv(system.file("data/subjectivity.csv.gz",
package = "sentiment"), header = FALSE, stringsAsFactors = FALSE)
#write.csv(OpinionFinder, file = "OpinionFinder")
OpinionFinder = as.data.frame(cbind(as.character(OpinionFinder$V1), as.integer(2*(OpinionFinder$V3 == "positive")-1)))
colnames(OpinionFinder) = c("word", "score")
OpinionFinder$score = as.integer(OpinionFinder$score)
OpinionFinder$score = (((OpinionFinder$score-1)*2)-1)*-1
#looking up Wiebe words
OpinionFinder[OpinionFinder$score == -1,]
OpinionFinder[pmatch("asu", OpinionFinder$word),] #the word "asu" matches "asunder" and gets a score of -1!!!
OpinionFinder[pmatch("you", OpinionFinder$word),]
#AFINN
AFINN = read.delim(file = "~/Desktop/Documents/GitRepos/Rsentiment/Lexicons/AFINN/AFINN-111.txt", stringsAsFactors = FALSE)
colnames(AFINN) = c("word", "score")
#looking up AFINN words
AFINN[AFINN$score == -5,]
AFINN[pmatch("am", AFINN$word),2]
AFINN[pmatch("feeling", AFINN$word),2]
AFINN[pmatch("creaking", AFINN$word),2]
AFINN[pmatch("plague", AFINN$word),2]
sent140$WIEBE.rating = classify.sentiment(sent140$clean, lexicon = Wiebe_lexicon)
sent140$emolex.rating = classify.sentiment(sent140$clean, lexicon = emolex)
sent140$OpinionFinder.rating = classify.sentiment(sent140$clean, lexicon = OpinionFinder)
sent140$OpinionFinder.rating.pred = sign(sent140$OpinionFinder.rating)
table(sent140$OpinionFinder.rating.pred)
table(sent140$polarity, sent140$OpinionFinder.rating.pred)
ans = table(sent140$polarity, sent140$OpinionFinder.rating.pred)
ans
accuracy = (ans["0","-1"]+ans["1","1"])/(sum(ans[,"-1"])+sum(ans[,"1"]))
accuracy #is 68.09% (excluding neutral tweets)
precision = ans["1","1"]/sum(ans[,"1"])
precision # precision is 67.46%
recall = ans["1","1"]/sum(ans["1",])
recall # recall is 48.47%
f1 = (2*precision*recall)/(precision + recall)
f1 #is 56.41%
emoticon$OpinionFinder.rating = classify.sentiment(emoticon$clean, lexicon = OpinionFinder)
emoticon$OpinionFinder.rating.pred = sign(emoticon$OpinionFinder.rating)
table(emoticon$AFINN.rating2.pred)
table(emoticon$AFINN.rating.pred)
ans = table(emoticon$polarity, emoticon$OpinionFinder.rating.pred)
ans
accuracy = (ans["0","-1"]+ans["1","1"])/(sum(ans[,"-1"])+sum(ans[,"1"]))
accuracy #is 75.87% (excluding neutral tweets)
precision = ans["1","1"]/sum(ans[,"1"])
precision # precision is 74.52%
recall = ans["1","1"]/sum(ans["1",])
recall # recall is 64.28%
f1 = (2*precision*recall)/(precision + recall)
f1 #is 69.02%
sent140$emolex.rating = classify.sentiment(sent140$clean, lexicon = emolex)
sent140$ANEW.rating = classify.sentiment(sent140$clean, lexicon = ANEW)
sent140$ANEW.rating.pred = sign(sent140$ANEW.rating)
table(sent140$ANEW.rating.pred)
ans = table(sent140$polarity, sent140$ANEW.rating.pred)
ans
emoticon$ANEW.rating = classify.sentiment(emoticon$clean, lexicon = ANEW)
accuracy = (ans["0","-1"]+ans["1","1"])/(sum(ans[,"-1"])+sum(ans[,"1"]))
accuracy #is 75.87% (excluding neutral tweets)
precision = ans["1","1"]/sum(ans[,"1"])
precision # precision is 74.52%
recall = ans["1","1"]/sum(ans["1",])
recall # recall is 64.28%
f1 = (2*precision*recall)/(precision + recall)
f1 #is 69.02%
emoticon$ANEW.rating = classify.sentiment(emoticon$clean, lexicon = ANEW)
head(ANEW)
ANEW = read.csv(file = "~/Desktop/Documents/GitRepos/Rsentiment/Lexicons/ANEW.csv", header = FALSE)
colnames(ANEW) = c("word", "score")
ANEW$score = ANEW$score - 5
range(ANEW$score)
ANEW$score = ANEW$score - 6
range(ANEW$score)
ANEW = read.csv(file = "~/Desktop/Documents/GitRepos/Rsentiment/Lexicons/ANEW.csv", header = FALSE)
colnames(ANEW) = c("word", "score")
#ANEW$score = ANEW$score-(mean(ANEW$score)+1) #normalize ANEW scores to 0. This didn't work as well as the next line did.
ANEW$score = ANEW$score - 6
range(ANEW$score)
head(ANEW[order(-ANEW$score),], 20) #happiest words
head(ANEW[order(ANEW$score),], 20) #saddest words
sent140$ANEW.rating = classify.sentiment(sent140$clean, lexicon = ANEW)
sent140$ANEW.rating.pred = sign(sent140$ANEW.rating)
table(sent140$ANEW.rating.pred)
ans = table(sent140$polarity, sent140$ANEW.rating.pred)
ans
accuracy = (ans["0","-1"]+ans["1","1"])/(sum(ans[,"-1"])+sum(ans[,"1"]))
accuracy #is 64.67% (excluding neutral tweets)
precision = ans["1","1"]/sum(ans[,"1"])
precision # precision is 59.74%
recall = ans["1","1"]/sum(ans["1",])
recall # recall is 90.65%
f1 = (2*precision*recall)/(precision + recall)
f1 #is 72.05%
emoticon$ANEW.rating = classify.sentiment(emoticon$clean, lexicon = ANEW)
emoticon$ANEW.rating.pred = sign(emoticon$ANEW.rating)
table(emoticon$AFINN.rating2.pred)
table(emoticon$AFINN.rating.pred)
table(emoticon$polarity, emoticon$ANEW.rating.pred)
ans
ans = table(emoticon$polarity, emoticon$ANEW.rating.pred)
ans
accuracy = (ans["0","-1"]+ans["1","1"])/(sum(ans[,"-1"])+sum(ans[,"1"]))
accuracy #is 74.07% (excluding neutral tweets)
precision = ans["1","1"]/sum(ans[,"1"])
precision # precision is 81.75%
recall = ans["1","1"]/sum(ans["1",])
recall # recall is 61.53%
f1 = (2*precision*recall)/(precision + recall)
f1 #is 70.21%
sent140$emolex.rating = classify.sentiment(sent140$clean, lexicon = emolex)
storage.directory
NRC = read.csv(file = paste(storage.directory,"Lexicons/EmoLex/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt", sep = ""),
sep = "\t", header = FALSE)
sent140$NRC.rating = classify.sentiment(sent140$clean, lexicon = NRC)
sent140$NRC.rating.pred = sign(sent140$NRC.rating)
table(sent140$NRC.rating.pred)
ans = table(sent140$polarity, sent140$NRC.rating.pred)
ans
ans = table(sent140$polarity, sent140$NRC.rating.pred)
ans
accuracy = (ans["0","-1"]+ans["1","1"])/(sum(ans[,"-1"])+sum(ans[,"1"]))
accuracy #is 75.87% (excluding neutral tweets)
precision = ans["1","1"]/sum(ans[,"1"])
precision # precision is 74.52%
recall = ans["1","1"]/sum(ans["1",])
recall # recall is 64.28%
f1 = (2*precision*recall)/(precision + recall)
f1 #is 69.02%
emoticon$NRC.rating = classify.sentiment(emoticon$clean, lexicon = NRC)
emoticon$NRC.rating.pred = sign(emoticon$NRC.rating)
table(emoticon$AFINN.rating2.pred)
ans = table(emoticon$polarity, emoticon$NRC.rating.pred)
ans
table(emoticon$AFINN.rating.pred)
ans = table(emoticon$polarity, emoticon$NRC.rating.pred)
ans
accuracy = (ans["0","-1"]+ans["1","1"])/(sum(ans[,"-1"])+sum(ans[,"1"]))
accuracy #is 75.87% (excluding neutral tweets)
precision = ans["1","1"]/sum(ans[,"1"])
precision # precision is 74.52%
precision # precision is 66.03%
recall = ans["1","1"]/sum(ans["1",])
recall # recall is 64.28%
f1 = (2*precision*recall)/(precision + recall)
f1 #is 69.02%
rm(term.freq)
rm(termFreq())
rm(term.freq.test)
rm(a)
rm(accuracy)
rm(alpha)
rm(ans)
rm(f1)
rm(inv.doc.freq)
rm(negations)
rm(precision)
rm(recall)
rm(NRC)
rm(OpinionFinder)
rm(emoticon.tf.idf)
rm(ANEW)
rm(emoticon)
rm(AFINN)
rm(freq.all)
rm(sent140)
rm(tf.idf)
source("functions.R")
dim(AFINN_lexicon)
