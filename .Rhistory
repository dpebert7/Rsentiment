return(cleantext)
}
sad_tweets$clean = clean.tweets(sad_tweets$text, usernameToken = "", hashToken = "")
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "")
#Create corpus using tm package
happy_corpus = Corpus(VectorSource(happy_tweets$clean))
sad_corpus = Corpus(VectorSource(sad_tweets$clean))
#Remove stop words
happy_corpus <- tm_map(happy_corpus, function(x)removeWords(x,stopwords())) # ~2 minutes
sad_corpus <- tm_map(sad_corpus, function(x)removeWords(x,stopwords())) # ~1 minute
#Make word cloud. Check reference for options
wordcloud(happy_corpus, max.words = 200) # ~4 minutes
wordcloud(sad_corpus, max.words = 200) # ~3 minutes
rm(AFINN_lexicon)
ls()
rm(list(ls()))
rm(as.list(ls()))
as.list(ls())
list(ls())
rm(c(ls()))
rm(as.list(ls()))
rm(as.character(ls()))
rm(as.list(as.character(ls())))
as.list(as.character(ls()))
vector(as.character(ls()))
as.avector(as.character(ls()))
as.vector(as.character(ls()))
rm(as.vector(as.character(ls())))
rm(AFINN_lexicon.frequencies())
rm(AFINN_lexicon.frequencies
)
rm(semi.clean.tweets())
rm(semi.clean.tweets)
rm(happy_emoticons)
rm(sad_corpus)
rm(happy_corpus)
rm(sad_tweets)
rm(happy_indices)
rm(happy_tweets)
rm(sad_emoticons)
rm(sad_indices)
rm(classify.sentiment)
rm(clean.tweets)
rm(ndsi.frequencies)
rm(word.freq)
happy = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
sad = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
# Clean tweets, remove blank tweets, then undersample from happy so that happy has as many tweets as sad does.
happy$clean = clean.tweets(happy$text, happyToken = "", sadToken = "") # do NOT tokenize :)
sad$clean = clean.tweets(sad$text, happyToken = "", sadToken = "")     # do NOT tokenize :(
happy$text = NULL
sad$text = NULL
happy  = happy[happy$clean!="",]
sad  = sad[sad$clean!="",]
set.seed(127)
index = sample(nrow(happy),nrow(sad))
happy = happy[index,]
dim(happy)
dim(sad) #both should have same number of rows
# Initialize polarity of happy and sad tweets
happy$polarity = as.factor(1)
sad$polarity = as.factor(0)
train = rbind(as.data.frame(happy[,c("clean", "polarity")]),sad[,c("clean", "polarity")])
colnames(train) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(train)
table(train$polarity)
library(stringr) #library for str_count function
library(e1071) # for naive bayes model
library(ggplot2) #for graphs
library(caret) #for confusionMatrix
library(pROC) #ROC curves
library(randomForest)
library(tm) # for building term frequency matrix from corpus
source("functions.R") #get cleaning function, AFINN_lexicon
# EMOTICON (TRAINING) DATA: semisuper tweets from LA county in 2014
happy = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
sad = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
# Clean tweets, remove blank tweets, then undersample from happy so that happy has as many tweets as sad does.
happy$clean = clean.tweets(happy$text, happyToken = "", sadToken = "") # do NOT tokenize :)
sad$clean = clean.tweets(sad$text, happyToken = "", sadToken = "")     # do NOT tokenize :(
happy$text = NULL
sad$text = NULL
happy  = happy[happy$clean!="",]
sad  = sad[sad$clean!="",]
set.seed(127)
index = sample(nrow(happy),nrow(sad))
happy = happy[index,]
dim(happy)
dim(sad) #both should have same number of rows
# Initialize polarity of happy and sad tweets
happy$polarity = as.factor(1)
sad$polarity = as.factor(0)
train = rbind(as.data.frame(happy[,c("clean", "polarity")]),sad[,c("clean", "polarity")])
colnames(train) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(train)
table(train$polarity)
# TEST/VALIDATION DATA: sentiment140
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test[test$polarity == 0,]$polarity = 0
test[test$polarity == 4,]$polarity = 1
table(test$polarity)
test = test[test$polarity !=2, c("polarity", "text")]
test$polarity = as.factor(test$polarity)
test$clean = clean.data(test$text)
test$clean = clean.tweets(test$text)
rm(test)
save("~/Desktop/Huang Research/Rsentiment/emoticon.RData") # save train/emoticon into memory as emoticon
save(emoticon, file = "~/Desktop/Huang Research/Rsentiment/emoticon.RData") # save train/emoticon into memory as emoticon
help(load)
dim(emoticon)
emoticon = train #rename train data frame to "emoticon"
rm(train)
dim(emoticon)
save(emoticon, file = "~/Desktop/Huang Research/Rsentiment/emoticon.RData") # save train/emoticon into memory as emoticon
rm(emoticon)
load("~/Desktop/Huang Research/Rsentiment/emoticon.RData") # load train/emoticon into memory as emoticon
dim(emoticon)
load("~/Desktop/Huang Research/Rsentiment/comTweetsLA.RData") # load LA2014 into memory as x
rm(x)
rm(happy)
rm(sad)
rm(AFINN_lexicon)
rm(index)
rm(emoticon)
library(stringr) #library for str_count function
library(e1071) # for naive bayes model
library(ggplot2) #for graphs
library(caret) #for confusionMatrix
library(pROC) #ROC curves
library(randomForest)
library(tm) # for building term frequency matrix from corpus
source("functions.R") #get cleaning function, AFINN_lexicon
load("~/Desktop/Huang Research/Rsentiment/emoticon.RData") # load train/emoticon into memory as emoticon
dim(emoticon)
table(emoticon$polarity)
head(word.freq(head(emoticon$clean,1000)), 100) # create word frequency data frame for first 1000 tweets
word.freq.pos = word.freq(emoticon$clean[emoticon$polarity == 1],
sparsity=0.9999) #terms must occur in at least 1 out of 1000 tweets
word.freq.neg = word.freq(emoticon$clean[emoticon$polarity == 0],
sparsity=0.9999)
dim(word.freq.pos)
dim(word.freq.neg)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
#Merge by word
freq.all = merge(word.freq.neg, word.freq.pos, by = 'word', all = T)
dim(word.freq.pos)
dim(word.freq.neg)
dim(freq.all)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
freq.all[1:20,]
#Set NA's to 0
freq.all$freq.x[is.na(freq.all$freq.x)] = 0
freq.all$freq.y[is.na(freq.all$freq.y)] = 0
#Differences between Positive and Negative Frequencies
freq.all$diff = abs(freq.all$freq.x - freq.all$freq.y)
head(freq.all[order(-freq.all$diff), ],100) #this is somewhat puzzling
#Smoothing term
alpha <- 2^7
#NDSI
freq.all$ndsi = abs(freq.all$freq.x -
freq.all$freq.y)/(freq.all$freq.x +
freq.all$freq.y +
2 * alpha)
#Sorting by NDSI
freq.all = freq.all[order(-freq.all$ndsi), ]
head(freq.all, 100)
#Convert word to a string
head(freq.all$word)
freq.all$word = as.character(freq.all$word)
head(freq.all$word,100)
freq.all = freq.all[freq.all$ndsi>=0.05,] # restrict to words with a decently high ndsi score.
# 0.05 might be too restrictive, but I want to keep the
# word list small to preserve memory
dim(freq.all)
head(freq.all)
head(freq.all[order(-freq.all$diff), ],100) #this is somewhat puzzling
dim(freq.all)
freq.all = merge(word.freq.neg, word.freq.pos, by = 'word', all = T)
dim(word.freq.pos)
dim(word.freq.neg)
dim(freq.all)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
freq.all[1:20,]
#Set NA's to 0
freq.all$freq.x[is.na(freq.all$freq.x)] = 0
freq.all$freq.y[is.na(freq.all$freq.y)] = 0
#Differences between Positive and Negative Frequencies
freq.all$diff = abs(freq.all$freq.x - freq.all$freq.y)
head(freq.all[order(-freq.all$diff), ],100) #this is somewhat puzzling
#Smoothing term
alpha <- 2^7
#NDSI
freq.all$ndsi = abs(freq.all$freq.x -
freq.all$freq.y)/(freq.all$freq.x +
freq.all$freq.y +
2 * alpha)
#Sorting by NDSI
freq.all = freq.all[order(-freq.all$ndsi), ]
head(freq.all, 100)
freq.all = freq.all[freq.all$ndsi>0,] # restrict to words with a nonzero ndsi score.
dim(freq.all)
dim(freq.all)
head(freq.all)
tail(freq.all)
storage.directory = "~/Desktop/Huang Research/Rsentiment/"
paste(storage.directory,"freq.all", sep = "")
save(freq.all, file = paste(storage.directory,"freq.all", sep = "")) # load train/emoticon into memory as emoticon
rm(freq.all)
load(paste(storage.directory,"freq.all", sep = "")) # load train/emoticon into memory as emoticon
tail(freq.all)
save(freq.all, file = paste(storage.directory,"freq.all.RData", sep = "")) # load train/emoticon into memory as emoticon
load(paste(storage.directory,"freq.all.RData", sep = "")) # load train/emoticon into memory as emoticon
load(paste(storage.directory,"freq.all", sep = "")) # load freq.all lexicon into memory as freq.all
head(freq.all)
load(paste(storage.directory,"freq.all.RData", sep = "")) # load freq.all lexicon into memory as freq.all
load(paste(storage.directory,"emoticon.RData", sep = "")) # load train/emoticon into memory as emoticon
save(emoticon, file = paste(storage.directory,"emoticon.RData", sep = "")) # save train/emoticon into memory as emoticon
load(paste(storage.directory,"emoticon.RData", sep = "")) # load train/emoticon into memory as emoticon
rm(emoticon)
load("~/Desktop/Huang Research/Rsentiment/emoticon.RData") # load train/emoticon into memory as emoticon
freq.all$word = as.character(freq.all$word)
load(paste(storage.directory,"freq.all.RData", sep = "")) # load freq.all lexicon into memory as freq.all
class(freq.all$word)
freq.all$word = as.character(freq.all$word)
save(freq.all, file = paste(storage.directory,"freq.all.RData", sep = "")) # save freq.all into memory as freq.all.RData
load(paste(storage.directory,"freq.all.RData", sep = "")) # load freq.all lexicon into memory as freq.all
class(freq.all$word)
head(freq.word)
head(freq.all)
dim(freq.all)
dim(freq.all[freq.all$ndsi>0.05])
dim(freq.all[freq.all$ndsi>0.05],)
dim(freq.all[freq.all$ndsi>0.05,])
freq.all = freq.all[freq.all$ndsi>0.05,]
dim(freq.all)
a = Sys.time()
term.freq <- t(apply(t(emoticon[,"clean"]), 2,    #TAKES TIME; 10 minute for 100 000 tweets and 1276 terms
ndsi.frequencies))
Sys.time()-a
inv.doc.freq=log(nrow(emoticon)/colSums(sign(term.freq)))
range(inv.doc.freq)
inv.doc.freq[is.infinite(inv.doc.freq)]=0
range(inv.doc.freq)
tf.idf = term.freq %*% diag(inv.doc.freq)
save(tf.idf, paste(storage.directory,"tf.idf.RData", sep = "")) # save tf.idf lexicon into memory as tf.idf
save(tf.idf, paste(storage.directory,"tf.idf.RData", sep = "")) # save tf.idf lexicon into memory as tf.idf
storage.directory = "~/Desktop/Huang Research/Rsentiment/"
save(tf.idf, paste(storage.directory,"tf.idf.RData", sep = "")) # save tf.idf lexicon into memory as tf.idf
paste(storage.directory, "tf.idf.RData)
""
)
)
paste(storage.directory, "tf.idf.RData", sep = "")
save(tf.idf, file = paste(storage.directory,"tf.idf.RData", sep = "")) # save tf.idf lexicon into memory as tf.idf
load(paste(storage.directory,"tf.idf.RData", sep = "")) # load tf.idf lexicon into memory as tf.idf
rm(freq.all)
rm(term.freq)
rm(word.freq.neg)
rm(word.freq.pos)
rm(AFINN_lexicon)
emoticon.tf.idf=data.frame(polarity=emoticon$polarity,tf.idf)
save(tf.idf, file = paste(storage.directory,"emoticon.tf.idf.RData", sep = "")) # save emoticon.tf.idf lexicon into memory as tf.idf
load(paste(storage.directory,"emoticon.tf.idf.RData", sep = "")) # load emoticon.tf.idf lexicon into memory as tf.idf
rm(tf.idf)
rm(emoticon)
rm(a)
rm(alpha)
rm(inv.doc.freq)
class(emoticon.tf.idf$polarity)
class(emoticon.tf.idf$X1)
class(emoticon.tf.idf$X2)
class(emoticon.tf.idf$X100)
class(emoticon.tf.idf$X1000)
help(svm)
source("functions.R") #get cleaning function, AFINN_lexicon
help(svm)
svm.model=svm(polarity~.,data = emoticon.tf.idf[1:100,])
svm.model=svm(polarity~.,data = emoticon.tf.idf[1:100,1:10])
help(apply)
apply(emoticon.tf.idf, 2, fun = sum)
apply(emoticon.tf.idf, 2, sum)
apply(emoticon.tf.idf[1:10,], 2, sum)
apply(emoticon.tf.idf[2:10,], 2, sum)
apply(emoticon.tf.idf[,2:10], 2, sum)
apply(emoticon.tf.idf[,2:100], 2, sum)
apply(emoticon.tf.idf[,2:1000], 2, sum)
apply(emoticon.tf.idf[,2:100], 2, sum)==0
table(apply(emoticon.tf.idf[,2:100], 2, sum)==0)
table(apply(emoticon.tf.idf[,2:1000], 2, sum)==0)
table(apply(emoticon.tf.idf[,2:nrow(emoticon.tf.idf)], 2, sum)==0)
table(apply(emoticon.tf.idf[,2:ncol(emoticon.tf.idf)], 2, sum)==0)
svm.model=svm(polarity~.,data = emoticon.tf.idf[1:100,1:10])
rf.idf[1,1]
sentiment.rf.idf[1,1]
emoticon.tf.idf[1,1]
emoticon.tf.idf[2,1]
emoticon.tf.idf[2,2]
svm.model=rpart(polarity~.,data = emoticon.tf.idf[1:100,1:10])
svm.model=svm(polarity~.,data = emoticon.tf.idf[1:100,1:10])
rf.model=randomForest(polarity~.,data = emoticon.tf.idf[1:10,1:100])
head(emoticon.tf.idf$polarity)
c(1:10,70000:70010)
dim(emoticon.tf.idf)
rf.model=randomForest(polarity~.,data = emoticon.tf.idf[c(1:10,70000:70010),])
rf.model
a = Sys.time()
rf.model=randomForest(polarity~.,data = emoticon.tf.idf[c(1:100,70000:70100),])
Sys.time()-a
rf.model
svm.model=svm(polarity~.,data = emoticon.tf.idf[c(1:10,70000:70010),])
a = Sys.time()
rf.model=randomForest(polarity~.,data = emoticon.tf.idf)
install.packages("openNLP")
load("~/Desktop/Huang Research/Rsentiment/comTweetsLA.RData") # load LA2014 into memory as x
x = x[1:10000,]
head(x$text)
textcat(head(x$text))
library(textcat)
textcat(head(x$text))
textcat(x$text)
x$text[9137]
textcat(x$text[9137])
x$text[9885]
textcat(x$text[9885])
x$text[9882:9883]
textcat(x$text[9882:9883])
library(cldr)
demo(cldr)
detectLanguage(x$text[1:100,])
detectLanguage(x$text[1:100])
detectLanguage(x$text[1:100])$detectedLanguage
x[detectLanguage(x$text[1:100])$detectedLanguage!="ENGLISH","text"]
x$language = detectLanguage(x$text)
x[,c("text", "language")]
x[1:"100",c("text", "language")]
x[1:100,c("text", "language")]
x[1:10,c("text", "language")]
x$language = detectLanguage(x$text)$languageDetected
x[1:10,c("text", "language")]
detectLanguage("hello world!")
x$language = detectLanguage(x$text)$detectedLanguage
x[1:10,c("text", "language")]
x[1:1000,c("text", "language")]
table(x[1:1000,c("text", "language")])
table(x$language)
table(x[x$language = "SPANISH",c("text", "language")])
table(x[x$language == "SPANISH",c("text", "language")])
detectLanguage("hello world!")
x$language = detectLanguage(x$text)$languageDetected
x$isReliable = detectLanguage(x$text)$isReliable
table(x$isReliable, x$language)
x$isReliable
table(x$isReliable)
table(x$language)
x$language = detectLanguage(x$text)$languageDetected
table(x$language)
x$language
x$language = detectLanguage(x$text)$detectedLanguage
table(x$language)
table(x$isReliable, x$language)
table(x[x$language != "ENGLISH"|x$isReliable = TRUE,c("text", "language")])
x[x$language != "ENGLISH"|x$isReliable = TRUE,c("text", "language")]
x[x$language != "ENGLISH" | x$isReliable == TRUE,c("text", "language")]
dim(x[x$language != "ENGLISH" | x$isReliable == TRUE, c("text", "language")])
dim(x[x$language != "ENGLISH" & x$isReliable == TRUE, c("text", "language")])
x[x$language != "ENGLISH" & x$isReliable == TRUE, c("text", "language")]
x$clean = clean.tweets(x$text)
x$language = detectLanguage(x$clean)$detectedLanguage
x$isReliable = detectLanguage(x$clean)$isReliable
table(x$isReliable, x$language)
dim(x[x$language != "ENGLISH" | x$isReliable == TRUE, c("text", "language")])
x[x$language != "ENGLISH" | x$isReliable == TRUE, c("text", "language")]
dim(x[x$language != "ENGLISH" & x$isReliable == TRUE, c("text", "language")])
x[x$language != "ENGLISH" & x$isReliable == TRUE, c("text", "language")]
x[9293,]
x[x$language == "SPANISH" & x$isReliable == TRUE, c("text", "language")]
dim(x[x$language != "ENGLISH" & x$isReliable == TRUE, c("clean", "language")])
x[x$language == "SPANISH" & x$isReliable == TRUE, c("clean", "language")]
dim(x[x$language != "ENGLISH" & x$isReliable == TRUE, c("clean", "language")])
x[x$language == "SPANISH" & x$isReliable == TRUE, c("clean", "language")]
x[x$language == "SPANISH" & x$isReliable == TRUE, c("clean", "language")]
x[x$language != "ENGLISH" & x$isReliable == TRUE, c("clean", "language")]
dim(x[x$language == "Unknown" & x$isReliable == TRUE, c("clean", "language")])
x[x$language == "Unknown" & x$isReliable == TRUE, c("clean", "language")]
dim(x[x$language != "ENGLISH" & x$isReliable == TRUE, c("clean", "language")]) #The blank and SPANISH tweets look like they could be removed, but other tweets look pretty English
x[x$language != "ENGLISH" & x$isReliable == TRUE, c("clean", "language")]
load("~/Desktop/Huang Research/Rsentiment/comTweetsLA.RData") # load LA2014 into memory as x
x$text<-as.character(x$text)
x$username<-as.character(x$username)
summary(x)
x[x$year == NA,]
name.count<-data.frame(count(x,username))
names(name.count)<-c("username","frequency")
name.count<-filter(name.count,username != "")
name.count<-arrange(name.count,desc(frequency))
hist(name.count$frequency)
hour.count<-data.frame(count(x,hour))
plot(hour.count$hour,hour.count$n)
month.count<-data.frame(count(x,month))
plot(month.count$month,month.count$n)
date.count<-data.frame(count(x,date))
plot(date.count$date,date.count$n)
dow.df <- data.frame(x$year,x$month,x$date)
names(dow.df) <- c("year","month","day")
dow.df$date <- paste(dow.df$year,dow.df$month,dow.df$day,sep = "-")
dow.df<-filter(dow.df, year != "NA")
dow.df<-filter(dow.df, month != "NA")
dow.df<-filter(dow.df, day != "NA")
dow.df$date <- as.Date(dow.df$date)
dow.df$wkday<-as.POSIXlt(dow.df$date)$wday
dow.count<-data.frame(count(dow.df,wkday))
plot(dow.count$wkday,dow.count$n)
plot(dow.count$wkday,dow.count$n)
hist(name.count$frequency)
plot(name.count$frequency)
hist(name.count$frequency)
hist(name.count$frequency)
plot(name.count$frequency)
plot(dow.count$wkday,dow.count$n)
plot(name.count$frequency)
plot(name.count$frequency,
log = "x")
plot(name.count$frequency,
log = "y")
plot(name.count$frequency,
log = "y",
xlab = "Unique Usernames",
ylab = "Number of Tweets")
plot(dow.count$wkday,dow.count$n)
plot(dow.count$wkday,dow.count$n,
xlab = "day of week"
ylab = "number of tweets")
plot(dow.count$wkday,dow.count$n,
xlab = "day of week",
ylab = "number of tweets")
rm(AFINN_lexicon)
rm(date.count)
rm(dow.count)
rm(dow.df)
rm(emoticon.tf.idf)
rm(hour.count)
rm(month.count)
rm(name.count)
rm(results)
rm(a)
rm(documents)
rm(rf.model)
rm(x)
plot(name.count$frequency,
log = "y",
xlab = "Unique Usernames",
ylab = "Number of Tweets")
name.count<-data.frame(count(x,username))
library(plyr)
library(dplyr)
load("~/Desktop/Huang Research/Rsentiment/comTweetsLA.RData") # load LA2014 into memory as x
library(plyr)
library(dplyr)
x$text<-as.character(x$text)
x$username<-as.character(x$username)
summary(x)
summary(x$username)
# Plot tweets per username
name.count<-data.frame(count(x,username))
# name.count$username<-as.character(name.count$username)
names(name.count)<-c("username","frequency")
name.count<-filter(name.count,username != "")
name.count<-arrange(name.count,desc(frequency))
hist(name.count$frequency)
plot(name.count$frequency,
log = "y",
xlab = "Unique Usernames",
ylab = "Number of Tweets")
line(name.count$frequency,
log = "y",
xlab = "Unique Usernames",
ylab = "Number of Tweets")
plot(name.count$frequency,
log = "y",
xlab = "Unique Usernames",
ylab = "Number of Tweets")
plot(name.count$frequency,
log = "y",
xlab = "Unique Usernames",
ylab = "Number of Tweets",
pch = 1)
plot(name.count$frequency,
log = "y",
xlab = "Unique Usernames",
ylab = "Number of Tweets",
pch = 4)
plot(name.count$frequency,
log = "y",
xlab = "Unique Usernames",
ylab = "Number of Tweets",
type = "l")
source("functions.R") #get cleaning function, AFINN_lexicon
install.packages("ggplot2")
source("functions.R") #get cleaning function, AFINN_lexicon
AFINN_lexicon = read.delim(file = "~/Desktop/Documents/GitRepos/Rsentiment/Lexicons/AFINN/AFINN-111.txt", stringsAsFactors = FALSE, header = F, quote = '')
AFINN_lexicon = read.delim(file = "~/Lexicons/AFINN/AFINN-111.txt", stringsAsFactors = FALSE, header = F, quote = '')
AFINN_lexicon = read.delim(file = "Lexicons/AFINN/AFINN-111.txt", stringsAsFactors = FALSE, header = F, quote = '')
head(AFINN_lexicon)
