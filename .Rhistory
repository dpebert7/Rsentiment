2+2
2+2
cleantext = laply(documents, function(documents)
{
documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("RT", "retweet", documents) # tokenize retweets
documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
if(keepHashText == TRUE){documents = gsub("#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.}
else{documents = gsub("#\\w+", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.}
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
clean.tweets = function(documents, usernameToken = "username", urlToken = "url", hashToken = "hash ", keepHashText = TRUE){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("RT", "retweet", documents) # tokenize retweets
documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
if(keepHashText == TRUE){
documents = gsub("#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
}
else{
documents = gsub("#\\w+", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
}
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
library(wordcloud)
library(tm)
source("functions.R")
library(wordcloud)
library(tm)
# load happy and sad tweets
happy_tweets = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", nrows = 150000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
dim(happy_tweets)
sad_tweets = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", nrows = 55000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
dim(sad_tweets)
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "", keepHashText = TRUE)
sad_tweets$clean = clean.tweets(sad_tweets$text, usernameToken = "", hashToken = "", keepHashText = TRUE)
clean.tweets = function(documents, usernameToken = "username", urlToken = "url", hashToken = "hash ", keepHashText = TRUE){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
documents = gsub("RT", "retweet", documents) # tokenize retweets
documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
if(keepHashText == TRUE){
documents = gsub("#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
}
else{
documents = gsub("#\\w+", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
}
documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "", keepHashText = TRUE)
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "", keepHashText = TRUE)
if(1==1){}
if(1==1){print("hello")}else{print("world")}
if(1!=1){print("hello")}else{print("world")}
gsub("#", "","#so happy #happy happy")
gsub("#\\w+", "","#so happy #happy happy")
gsub("#", "","#so happy #happy happy")
clean.tweets = function(documents, usernameToken = "username", urlToken = "url", hashToken = "hash ", keepHashText = TRUE){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
documents = gsub("RT", "retweet", documents) # tokenize retweets
documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
#if(keepHashText == TRUE){
#  documents = gsub("\\#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
#  }
#else{
#  documents = gsub("#\\w+", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
#}
documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
sad_tweets$clean = clean.tweets(sad_tweets$text, usernameToken = "", hashToken = "", keepHashText = TRUE)
clean.tweets = function(documents, usernameToken = "username", urlToken = "url", hashToken = "hash "){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
documents = gsub("RT", "retweet", documents) # tokenize retweets
documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
documents = gsub("\\#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
sad_tweets$clean = clean.tweets(sad_tweets$text, usernameToken = "", hashToken = "")
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "")
happy_tweets$text
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "")
clean.tweets = function(documents, usernameToken = "username", urlToken = "url", hashToken = "hash "){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
documents = gsub("RT", "retweet", documents) # tokenize retweets
#documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
documents = gsub("\\#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
#documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
#documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "")
source("functions.R")
happy_corpus = Corpus(VectorSource(happy_tweets$clean))
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "")
clean.tweets = function(documents, usernameToken = "username", urlToken = "url", hashToken = "hash "){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
documents = gsub("RT", "retweet", documents) # tokenize retweets
#documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
documents = gsub("\\#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "")
happy_tweets$text[1:1000]
happy_tweets$clean[1:1000]
clean.tweets = function(documents, usernameToken = "username", hashToken = "hash "){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
#documents = gsub("RT", "retweet", documents) # tokenize retweets. Ignore this since tweets aren't retweeets
documents = rm_url(documents) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
documents = gsub("\\#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
documents = gsub(paste(sad_emoticons, collapse = "|"), " sademoticon ", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), " happyemoticon ", documents) #tokenize happy emoticons
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
sad_tweets$clean = clean.tweets(sad_tweets$text, usernameToken = "", hashToken = "")
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "")
#Create corpus using tm package
happy_corpus = Corpus(VectorSource(happy_tweets$clean))
sad_corpus = Corpus(VectorSource(sad_tweets$clean))
#Remove stop words
happy_corpus <- tm_map(happy_corpus, function(x)removeWords(x,stopwords())) # ~2 minutes
sad_corpus <- tm_map(sad_corpus, function(x)removeWords(x,stopwords())) # ~1 minute
#Make word cloud. Check reference for options
wordcloud(happy_corpus, max.words = 200) # ~4 minutes
wordcloud(sad_corpus, max.words = 200) # ~3 minutes
rm(AFINN_lexicon)
ls()
rm(list(ls()))
rm(as.list(ls()))
as.list(ls())
list(ls())
rm(c(ls()))
rm(as.list(ls()))
rm(as.character(ls()))
rm(as.list(as.character(ls())))
as.list(as.character(ls()))
vector(as.character(ls()))
as.avector(as.character(ls()))
as.vector(as.character(ls()))
rm(as.vector(as.character(ls())))
rm(AFINN_lexicon.frequencies())
rm(AFINN_lexicon.frequencies
)
rm(semi.clean.tweets())
rm(semi.clean.tweets)
rm(happy_emoticons)
rm(sad_corpus)
rm(happy_corpus)
rm(sad_tweets)
rm(happy_indices)
rm(happy_tweets)
rm(sad_emoticons)
rm(sad_indices)
rm(classify.sentiment)
rm(clean.tweets)
rm(ndsi.frequencies)
rm(word.freq)
happy = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
sad = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
# Clean tweets, remove blank tweets, then undersample from happy so that happy has as many tweets as sad does.
happy$clean = clean.tweets(happy$text, happyToken = "", sadToken = "") # do NOT tokenize :)
sad$clean = clean.tweets(sad$text, happyToken = "", sadToken = "")     # do NOT tokenize :(
happy$text = NULL
sad$text = NULL
happy  = happy[happy$clean!="",]
sad  = sad[sad$clean!="",]
set.seed(127)
index = sample(nrow(happy),nrow(sad))
happy = happy[index,]
dim(happy)
dim(sad) #both should have same number of rows
# Initialize polarity of happy and sad tweets
happy$polarity = as.factor(1)
sad$polarity = as.factor(0)
train = rbind(as.data.frame(happy[,c("clean", "polarity")]),sad[,c("clean", "polarity")])
colnames(train) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(train)
table(train$polarity)
library(stringr) #library for str_count function
library(e1071) # for naive bayes model
library(ggplot2) #for graphs
library(caret) #for confusionMatrix
library(pROC) #ROC curves
library(randomForest)
library(tm) # for building term frequency matrix from corpus
source("functions.R") #get cleaning function, AFINN_lexicon
# EMOTICON (TRAINING) DATA: semisuper tweets from LA county in 2014
happy = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
sad = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
# Clean tweets, remove blank tweets, then undersample from happy so that happy has as many tweets as sad does.
happy$clean = clean.tweets(happy$text, happyToken = "", sadToken = "") # do NOT tokenize :)
sad$clean = clean.tweets(sad$text, happyToken = "", sadToken = "")     # do NOT tokenize :(
happy$text = NULL
sad$text = NULL
happy  = happy[happy$clean!="",]
sad  = sad[sad$clean!="",]
set.seed(127)
index = sample(nrow(happy),nrow(sad))
happy = happy[index,]
dim(happy)
dim(sad) #both should have same number of rows
# Initialize polarity of happy and sad tweets
happy$polarity = as.factor(1)
sad$polarity = as.factor(0)
train = rbind(as.data.frame(happy[,c("clean", "polarity")]),sad[,c("clean", "polarity")])
colnames(train) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(train)
table(train$polarity)
# TEST/VALIDATION DATA: sentiment140
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test[test$polarity == 0,]$polarity = 0
test[test$polarity == 4,]$polarity = 1
table(test$polarity)
test = test[test$polarity !=2, c("polarity", "text")]
test$polarity = as.factor(test$polarity)
test$clean = clean.data(test$text)
test$clean = clean.tweets(test$text)
rm(test)
save("~/Desktop/Huang Research/Rsentiment/emoticon.RData") # save train/emoticon into memory as emoticon
save(emoticon, file = "~/Desktop/Huang Research/Rsentiment/emoticon.RData") # save train/emoticon into memory as emoticon
help(load)
dim(emoticon)
emoticon = train #rename train data frame to "emoticon"
rm(train)
dim(emoticon)
save(emoticon, file = "~/Desktop/Huang Research/Rsentiment/emoticon.RData") # save train/emoticon into memory as emoticon
rm(emoticon)
load("~/Desktop/Huang Research/Rsentiment/emoticon.RData") # load train/emoticon into memory as emoticon
dim(emoticon)
load("~/Desktop/Huang Research/Rsentiment/comTweetsLA.RData") # load LA2014 into memory as x
rm(x)
rm(happy)
rm(sad)
rm(AFINN_lexicon)
rm(index)
rm(emoticon)
library(stringr) #library for str_count function
library(e1071) # for naive bayes model
library(ggplot2) #for graphs
library(caret) #for confusionMatrix
library(pROC) #ROC curves
library(randomForest)
library(tm) # for building term frequency matrix from corpus
source("functions.R") #get cleaning function, AFINN_lexicon
load("~/Desktop/Huang Research/Rsentiment/emoticon.RData") # load train/emoticon into memory as emoticon
dim(emoticon)
table(emoticon$polarity)
head(word.freq(head(emoticon$clean,1000)), 100) # create word frequency data frame for first 1000 tweets
word.freq.pos = word.freq(emoticon$clean[emoticon$polarity == 1],
sparsity=0.9999) #terms must occur in at least 1 out of 1000 tweets
word.freq.neg = word.freq(emoticon$clean[emoticon$polarity == 0],
sparsity=0.9999)
dim(word.freq.pos)
dim(word.freq.neg)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
#Merge by word
freq.all = merge(word.freq.neg, word.freq.pos, by = 'word', all = T)
dim(word.freq.pos)
dim(word.freq.neg)
dim(freq.all)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
freq.all[1:20,]
#Set NA's to 0
freq.all$freq.x[is.na(freq.all$freq.x)] = 0
freq.all$freq.y[is.na(freq.all$freq.y)] = 0
#Differences between Positive and Negative Frequencies
freq.all$diff = abs(freq.all$freq.x - freq.all$freq.y)
head(freq.all[order(-freq.all$diff), ],100) #this is somewhat puzzling
#Smoothing term
alpha <- 2^7
#NDSI
freq.all$ndsi = abs(freq.all$freq.x -
freq.all$freq.y)/(freq.all$freq.x +
freq.all$freq.y +
2 * alpha)
#Sorting by NDSI
freq.all = freq.all[order(-freq.all$ndsi), ]
head(freq.all, 100)
#Convert word to a string
head(freq.all$word)
freq.all$word = as.character(freq.all$word)
head(freq.all$word,100)
freq.all = freq.all[freq.all$ndsi>=0.05,] # restrict to words with a decently high ndsi score.
# 0.05 might be too restrictive, but I want to keep the
# word list small to preserve memory
dim(freq.all)
head(freq.all)
head(freq.all[order(-freq.all$diff), ],100) #this is somewhat puzzling
dim(freq.all)
freq.all = merge(word.freq.neg, word.freq.pos, by = 'word', all = T)
dim(word.freq.pos)
dim(word.freq.neg)
dim(freq.all)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
freq.all[1:20,]
#Set NA's to 0
freq.all$freq.x[is.na(freq.all$freq.x)] = 0
freq.all$freq.y[is.na(freq.all$freq.y)] = 0
#Differences between Positive and Negative Frequencies
freq.all$diff = abs(freq.all$freq.x - freq.all$freq.y)
head(freq.all[order(-freq.all$diff), ],100) #this is somewhat puzzling
#Smoothing term
alpha <- 2^7
#NDSI
freq.all$ndsi = abs(freq.all$freq.x -
freq.all$freq.y)/(freq.all$freq.x +
freq.all$freq.y +
2 * alpha)
#Sorting by NDSI
freq.all = freq.all[order(-freq.all$ndsi), ]
head(freq.all, 100)
freq.all = freq.all[freq.all$ndsi>0,] # restrict to words with a nonzero ndsi score.
dim(freq.all)
dim(freq.all)
head(freq.all)
tail(freq.all)
storage.directory = "~/Desktop/Huang Research/Rsentiment/"
paste(storage.directory,"freq.all", sep = "")
save(freq.all, file = paste(storage.directory,"freq.all", sep = "")) # load train/emoticon into memory as emoticon
rm(freq.all)
load(paste(storage.directory,"freq.all", sep = "")) # load train/emoticon into memory as emoticon
tail(freq.all)
save(freq.all, file = paste(storage.directory,"freq.all.RData", sep = "")) # load train/emoticon into memory as emoticon
load(paste(storage.directory,"freq.all.RData", sep = "")) # load train/emoticon into memory as emoticon
load(paste(storage.directory,"freq.all", sep = "")) # load freq.all lexicon into memory as freq.all
head(freq.all)
load(paste(storage.directory,"freq.all.RData", sep = "")) # load freq.all lexicon into memory as freq.all
load(paste(storage.directory,"emoticon.RData", sep = "")) # load train/emoticon into memory as emoticon
save(emoticon, file = paste(storage.directory,"emoticon.RData", sep = "")) # save train/emoticon into memory as emoticon
load(paste(storage.directory,"emoticon.RData", sep = "")) # load train/emoticon into memory as emoticon
rm(emoticon)
load("~/Desktop/Huang Research/Rsentiment/emoticon.RData") # load train/emoticon into memory as emoticon
freq.all$word = as.character(freq.all$word)
load(paste(storage.directory,"freq.all.RData", sep = "")) # load freq.all lexicon into memory as freq.all
class(freq.all$word)
freq.all$word = as.character(freq.all$word)
save(freq.all, file = paste(storage.directory,"freq.all.RData", sep = "")) # save freq.all into memory as freq.all.RData
load(paste(storage.directory,"freq.all.RData", sep = "")) # load freq.all lexicon into memory as freq.all
class(freq.all$word)
head(freq.word)
head(freq.all)
dim(freq.all)
dim(freq.all[freq.all$ndsi>0.05])
dim(freq.all[freq.all$ndsi>0.05],)
dim(freq.all[freq.all$ndsi>0.05,])
freq.all = freq.all[freq.all$ndsi>0.05,]
dim(freq.all)
a = Sys.time()
term.freq <- t(apply(t(emoticon[,"clean"]), 2,    #TAKES TIME; 10 minute for 100 000 tweets and 1276 terms
ndsi.frequencies))
Sys.time()-a
inv.doc.freq=log(nrow(emoticon)/colSums(sign(term.freq)))
range(inv.doc.freq)
inv.doc.freq[is.infinite(inv.doc.freq)]=0
range(inv.doc.freq)
tf.idf = term.freq %*% diag(inv.doc.freq)
save(tf.idf, paste(storage.directory,"tf.idf.RData", sep = "")) # save tf.idf lexicon into memory as tf.idf
save(tf.idf, paste(storage.directory,"tf.idf.RData", sep = "")) # save tf.idf lexicon into memory as tf.idf
storage.directory = "~/Desktop/Huang Research/Rsentiment/"
save(tf.idf, paste(storage.directory,"tf.idf.RData", sep = "")) # save tf.idf lexicon into memory as tf.idf
paste(storage.directory, "tf.idf.RData)
""
)
)
paste(storage.directory, "tf.idf.RData", sep = "")
save(tf.idf, file = paste(storage.directory,"tf.idf.RData", sep = "")) # save tf.idf lexicon into memory as tf.idf
load(paste(storage.directory,"tf.idf.RData", sep = "")) # load tf.idf lexicon into memory as tf.idf
rm(freq.all)
rm(term.freq)
rm(word.freq.neg)
rm(word.freq.pos)
rm(AFINN_lexicon)
emoticon.tf.idf=data.frame(polarity=emoticon$polarity,tf.idf)
save(tf.idf, file = paste(storage.directory,"emoticon.tf.idf.RData", sep = "")) # save emoticon.tf.idf lexicon into memory as tf.idf
load(paste(storage.directory,"emoticon.tf.idf.RData", sep = "")) # load emoticon.tf.idf lexicon into memory as tf.idf
rm(tf.idf)
rm(emoticon)
rm(a)
rm(alpha)
rm(inv.doc.freq)
