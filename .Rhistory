#70% training data ----
set.seed(945)
train = sample(1:nrow(semisuper), 0.7*nrow(semisuper))
word.freq <- function(document.vector, sparsity = .99){
head(word.freq(semisuper$clean), 100)
train.data=semisuper[train,]
word.freq.pos = word.freq(train.data$clean[train.data$polarity == 1],
sparsity=0.999)
word.freq.neg = word.freq(train.data$clean[train.data$polarity == -1],
sparsity=0.999)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
#Merge by word
freq.all = merge(word.freq.neg, word.freq.pos, by = 'word', all = T)
dim(word.freq.pos)
dim(word.freq.neg)
dim(freq.all)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
freq.all[1:20,]
)
head(word.freq(semisuper$clean), 100)
train.data=semisuper[train,]
word.freq.pos = word.freq(train.data$clean[train.data$polarity == 1],
sparsity=0.999)
word.freq.neg = word.freq(train.data$clean[train.data$polarity == -1],
sparsity=0.999)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
#Merge by word
freq.all = merge(word.freq.neg, word.freq.pos, by = 'word', all = T)
dim(word.freq.pos)
dim(word.freq.neg)
dim(freq.all)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
freq.all[1:20,]
#Set NA's to 0
freq.all$freq.x[is.na(freq.all$freq.x)] = 0
freq.all$freq.y[is.na(freq.all$freq.y)] = 0
#Differences between Positive and Negative Frequencies
freq.all$diff = abs(freq.all$freq.x - freq.all$freq.y)
head(freq.all[order(-freq.all$diff), ])
#Smoothing term
alpha <- 2^7
#NDSI
freq.all$ndsi = abs(freq.all$freq.x -
freq.all$freq.y)/(freq.all$freq.x +
freq.all$freq.y +
2 * alpha)
#Sorting by NDSI
freq.all = freq.all[order(-freq.all$ndsi), ]
head(freq.all, 100)
#Convert word to a string
head(freq.all$word)
freq.all$word = as.character(freq.all$word)
head(freq.all$word)
# Term Frequencies and tfidf with NDSI ----
#AFINN Frequency Function (now used with ndsi lexicon)
freq.all$word = as.character(freq.all$word)
library(stringr)
ndsi.frequencies=function(x){
str_count(x,freq.all$word[1:1024])
}
#Term Frequencies (Takes about two minutes to run)
term.freq <- t(apply(t(semisuper$clean), 2,
ndsi.frequencies))
inv.doc.freq=log(nrow(semisuper)/colSums(sign(term.freq)))
range(inv.doc.freq)
inv.doc.freq[is.infinite(inv.doc.freq)]=0
range(inv.doc.freq)
tf.idf = term.freq %*% diag(inv.doc.freq)
# Random Forest Using NDSI tf.idf ----
semisuper$polarity=as.factor(semisuper$polarity)
rf.semisuper=data.frame(polarity=semisuper$polarity,tf.idf)
#Random Forest (Takes about 15 min to run)
rf.model=randomForest(sentiment~.,data=rf.semisuper[train,])
rf.model=randomForest(polarity~.,data=rf.semisuper[train,])
View(train.data)
pred.polarity=predict(rf.model,
newdata=rf.movie.data[-train,])
pred.polarity=predict(rf.model,
newdata=rf.semisuper[-train,])
confusionMatrix(pred.polarity,movie.data$polarity[-train])
confusionMatrix(pred.polarity,semisuper$polarity[-train])
phat=predict(rf.model,
newdata=rf.movie.data[-train,],
type="prob")
phat=predict(rf.model,
newdata=rf.semisuper[-train,],
type="prob")
library(pROC)
plot(roc(movie.data$polarity[-train],phat[,2]))
library(pROC)
plot(roc(rf.semisuper$polarity[-train],phat[,2]))
happy = read.csv("happy_tweets.csv", stringsAsFactors = FALSE)
sad = read.csv("sad_tweets.csv", stringsAsFactors = FALSE)
happy$polarity = 1
sad$polarity = -1
semisuper = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
semisuper$clean = clean.data(semisuper$text)
dim(semisuper)
table(semisuper$polarity)
set.seed(945)
train = sample(1:nrow(semisuper), 0.7*nrow(semisuper))
train.data=semisuper[train,]
word.freq.pos = word.freq(train.data$clean[train.data$polarity == 1],
sparsity=0.999)
word.freq.neg = word.freq(train.data$clean[train.data$polarity == -1],
sparsity=0.999)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
freq.all = merge(word.freq.neg, word.freq.pos, by = 'word', all = T)
dim(word.freq.pos)
dim(word.freq.neg)
dim(freq.all)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
freq.all[1:20,]
freq.all$freq.x[is.na(freq.all$freq.x)] = 0
freq.all$freq.y[is.na(freq.all$freq.y)] = 0
freq.all$diff = abs(freq.all$freq.x - freq.all$freq.y)
head(freq.all[order(-freq.all$diff), ])
alpha <- 2^7
freq.all$ndsi = abs(freq.all$freq.x -
freq.all$freq.y)/(freq.all$freq.x +
freq.all$freq.y +
2 * alpha)
freq.all = freq.all[order(-freq.all$ndsi), ]
head(freq.all, 100)
head(freq.all$word)
freq.all$word = as.character(freq.all$word)
head(freq.all$word)
# Term Frequencies and tfidf with NDSI ----
#AFINN Frequency Function (now used with ndsi lexicon)
freq.all$word = as.character(freq.all$word)
library(stringr)
ndsi.frequencies=function(x){
str_count(x,freq.all$word[1:1024])
}
term.freq <- t(apply(t(semisuper$clean), 2,
ndsi.frequencies))
inv.doc.freq=log(nrow(semisuper)/colSums(sign(term.freq)))
range(inv.doc.freq)
inv.doc.freq[is.infinite(inv.doc.freq)]=0
range(inv.doc.freq)
tf.idf = term.freq %*% diag(inv.doc.freq)
# Random Forest Using NDSI tf.idf ----
semisuper$polarity=as.factor(semisuper$polarity)
rf.semisuper=data.frame(polarity=semisuper$polarity,tf.idf)
View(rf.semisuper)
dim(rf.semisuper)
#Random Forest (Takes about 15 min to run)
rf.model=randomForest(polarity~.,data=rf.semisuper[train,])
#Classification Accuracy
pred.polarity=predict(rf.model,
newdata=rf.semisuper[-train,])
confusionMatrix(pred.polarity,semisuper$polarity[-train])
#ROC Curve
phat=predict(rf.model,
newdata=rf.semisuper[-train,],
type="prob")
library(pROC)
plot(roc(rf.semisuper$polarity[-train],phat[,2]))
happy = read.csv("happy_tweets2016.csv", stringsAsFactors = FALSE)
sad = read.csv("sad_tweets2016.csv", stringsAsFactors = FALSE)
happy$polarity = 1
sad$polarity = -1
semisuper = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
semisuper$clean = clean.data(semisuper$text)
dim(semisuper)
rm(happy_tweets)
rm(sad_tweets)
rm(happy_corpus)
rm(sad_corpus)
rm(happy_indices)
rm(sad_indices)
library(stringr) #library for str_count function
library(e1071) # for naive bayes model
library(ggplot2) #for graphs
library(caret) #for confusionMatrix
library(pROC) #ROC curves
library(randomForest)
library(tm) # for building term frequency matrix from corpus
source("functions.R") #get cleaning function, AFINN_lexicon
happy = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", nrows = 110000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
dim(happy)
sad = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", nrows = 50000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
dim(sad)
happy$text = clean.data(happy$text)
sad$text = clean.data(sad$text)
happy  = happy[happy$text!="",]
sad  = sad[sad$text!="",]
set.seed(127)
index = sample(nrow(happy),nrow(sad))
happy = happy[index,]
dim(happy)
dim(sad) #both should have 48124 rows
happy$polarity = 1
sad$polarity = -1
semisuper = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
colnames(semisuper) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(semisuper)
table(semisuper$polarity)
happy = happy[1:1000,]
sad = sad[1:1000,]
dim(happy)
dim(sad)
happy$polarity = 1
sad$polarity = -1
semisuper = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
colnames(semisuper) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(semisuper)
table(semisuper$polarity)
word.freq <- function(document.vector, sparsity = .99){
# construct corpus
temp.corpus <- Corpus(VectorSource(document.vector))
# construct tf matrix and remove sparse terms
temp.tf <- DocumentTermMatrix(temp.corpus,
control = list(stopwords = stopwords('english'),
removeNumbers = T))
temp.tf <- removeSparseTerms(temp.tf, sparsity)
temp.tf <- as.matrix(temp.tf)
# construct word frequency df
freq.df <- colSums(temp.tf)
freq.df <- data.frame(word = names(freq.df), freq = freq.df)
rownames(freq.df) <- NULL
return(freq.df)
}
head(word.freq(semisuper$clean), 100)
word.freq.pos = word.freq(semisuper$clean[semisuper$polarity == 1],
sparsity=0.9999) #terms must occur in at least 1 out of 1000
word.freq.neg = word.freq(semisuper$clean[semisuper$polarity == -1],
sparsity=0.9999)
dim(word.freq.pos)
dim(word.freq.neg)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
#Merge by word
freq.all = merge(word.freq.neg, word.freq.pos, by = 'word', all = T)
dim(word.freq.pos)
dim(word.freq.neg)
dim(freq.all)
word.freq.pos[1:20,]
word.freq.neg[1:20,]
freq.all[1:20,]
#Set NA's to 0
freq.all$freq.x[is.na(freq.all$freq.x)] = 0
freq.all$freq.y[is.na(freq.all$freq.y)] = 0
#Differences between Positive and Negative Frequencies
freq.all$diff = abs(freq.all$freq.x - freq.all$freq.y)
head(freq.all[order(-freq.all$diff), ])
#Smoothing term
alpha <- 2^7
#NDSI
freq.all$ndsi = abs(freq.all$freq.x -
freq.all$freq.y)/(freq.all$freq.x +
freq.all$freq.y +
2 * alpha)
#Sorting by NDSI
freq.all = freq.all[order(-freq.all$ndsi), ]
head(freq.all, 100)
#Convert word to a string
head(freq.all$word)
freq.all$word = as.character(freq.all$word)
head(freq.all$word)
# Term Frequencies and tfidf with NDSI ----
#AFINN Frequency Function (now used with ndsi lexicon)
freq.all$word = as.character(freq.all$word)
library(stringr)
ndsi.frequencies=function(x){
str_count(x,freq.all$word[1:1024]) #1024 words with highest NDSI score
}
length(freq.all$word)
ndsi.frequencies=function(x){
str_count(x,freq.all$word[1:1024]) #1024 words with highest NDSI score
}
term.freq <- t(apply(t(semisuper$clean), 2,
ndsi.frequencies))
inv.doc.freq=log(nrow(semisuper)/colSums(sign(term.freq)))
range(inv.doc.freq)
inv.doc.freq[is.infinite(inv.doc.freq)]=0
range(inv.doc.freq)
tf.idf = term.freq %*% diag(inv.doc.freq)
semisuper$polarity=as.factor(semisuper$polarity)
rf.semisuper=data.frame(polarity=semisuper$polarity,tf.idf)
library(rpart)
tree = rpart(polarity~., data = rf.semisuper)
pred.sentiment=predict(tree,
newdata=rf.movie.data[-train,])
tree
predict(tree, newdata = semisuper, type = "prob")
predict(tree, newdata = semisuper)
help(predict)
predict(tree, newdata = rf.semisuper)
predict(tree, newdata = rf.semisuper, type = "class")
table(predict(tree, newdata = rf.semisuper, type = "class"))
tree
library(ctree)
print("hello world")
library(stringr) #library for str_count function
library(e1071) # for naive bayes model
library(ggplot2) #for graphs
library(caret) #for confusionMatrix
library(pROC) #ROC curves
library(randomForest)
library(tm) # for building term frequency matrix from corpus
source("functions.R") #get cleaning function, AFINN_lexicon
happy = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", nrows = 110000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
sad = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", nrows = 50000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
#Clean tweets, remove blank tweets, then undersample from happy so that happy has as many tweets as sad does.
happy$text = clean.data(happy$text)
sad$text = clean.data(sad$text)
happy  = happy[happy$text!="",]
sad  = sad[sad$text!="",]
set.seed(127)
index = sample(nrow(happy),nrow(sad))
happy = happy[index,]
dim(happy)
dim(sad) #both should have 48124 rows
happy$polarity = 1
sad$polarity = -1
semisuper = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
colnames(semisuper) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(semisuper)
table(semisuper$polarity)
val = read.csv("testdata.manual.2009.06.14.csv")
dim(val)
head(val)
head(val,2)
rm(val)
val[10:15]
test[10:15,]
test = read.csv("testdata.manual.2009.06.14.csv")
test[10:15,]
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE)
head(test)
test
colnames(test) = c("polarity", "not_sure", "time", "search_query", "username", "text")
head(test)
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("integer", "integer", "character", "character", "character", "string"))
c("numeric", "integer", "character", "character", "character", "string"))
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("numeric", "integer", "character", "character", "character", "string"))
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("text", "text", "character", "character", "character", "string"))
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "time", "search_query", "username", "text")
test$polarity = as.factor(test$polarity)
test$time = as.POSIXct(test$time)
test$time = as.POSIXlt.factor(test$time)
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test$polarity = as.factor(test$polarity)
test$polarity = as.factor(test$polarity)
happy$polarity = 1
sad$polarity = 0
semisuper = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
colnames(semisuper) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(semisuper)
table(semisuper$polarity)
class(semisuper$polarity)
happy = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", nrows = 110000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
sad = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", nrows = 50000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
#Clean tweets, remove blank tweets, then undersample from happy so that happy has as many tweets as sad does.
happy$text = clean.data(happy$text)
sad$text = clean.data(sad$text)
happy  = happy[happy$text!="",]
sad  = sad[sad$text!="",]
happy$polarity = as.factor(1)
sad$polarity = as.factor(0)
train = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
class(train$polarity)
train = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
colnames(train) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(train)
table(train$polarity)
set.seed(127)
index = sample(nrow(happy),nrow(sad))
happy = happy[index,]
dim(happy)
dim(sad) #both should have 48124 rows
happy$polarity = as.factor(1)
sad$polarity = as.factor(0)
train = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
colnames(train) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(train)
table(train$polarity)
test[test$polarity == 4, ]$polarity = as.factor(1)
test[test$polarity == 4,]
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test[1:10,]
test$polarity == 4
test[test$polarity == 4,]
test[test$polarity == 4,]$polarity
test[test$polarity == 4,]$polarity = as.factor(1)
test[test$polarity == 2,]$polarity = as.factor(NA)
test[test$polarity == 0,]$polarity = as.factor(0)
test[test$polarity == 0,]$polarity = as.factor(0)
test$polarity == 0
happy$polarity = as.factor(1)
sad$polarity = as.factor(0)
train = rbind(as.data.frame(happy[,c("text", "polarity")]),sad[,c("text", "polarity")])
colnames(train) = c("clean", "polarity") #note that cleaning already took place, so these column names are appropriate
dim(train)
table(train$polarity)
# validation data: sentiment140
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test[test$polarity == 4,]$polarity = as.factor(1)
test[test$polarity == 0,]$polarity = as.factor(0)
test$polarity = as.factor(test$polarity)
head(test)
head(test, 200)
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
head(test, 200)
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test[test$polarity == 0,]$polarity = as.factor(0)
test[test$polarity == 4,]$polarity = as.factor(1)
table(test$polarity)
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
table(test$polarity)
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
table(test$polarity)
test[test$polarity == 0,]$polarity = 0
test[test$polarity == 4,]$polarity = 1
table(test$polarity)
table(test[test$polarity !=2,])
table(test[test$polarity !=2,]$polarity)
test = test[test$polarity !=2, c("polarity", "text")]
test
x = read.csv(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.csv", nrows = 9400000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
length(grep("=\\)", x$text, value = TRUE)) # =)'s in the whole set
length(grep("\\(=", x$text, value = TRUE)) # (='s in the whole set
grep("\\(=", x$text, value = TRUE) #195 (='s in the whole set
grep("=\\)", x$text, value = TRUE) #1467 =)'s in the whole set
grep("☺", x$text, value = TRUE) #1467 =)'s in the whole set
length(grep("=\\(", x$text, value = TRUE)) # =('s in the whole set
length(grep("\\)=", x$text, value = TRUE)) # )='s in the whole set
length(grep("\\)=", x$text, value = TRUE)) # )='s in the whole set
grep("=\\(", x$text, value = TRUE) #59 =('s in the whole set
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test[test$polarity == 0,]$polarity = 0
test[test$polarity == 4,]$polarity = 1
table(test$polarity)
test = test[test$polarity !=2, c("polarity", "text")]
test$polarity = as.factor(test$polarity)
head(test, 200)
test$clean = clean.data(test$text)
a = Sys.time()
term.freq <- t(apply(t(test$clean), 2,
AFINN_lexicon.frequencies))
Sys.time()-a
dim(term.freq)
semisuper$AFINN.rating = as.vector(term.freq %*% AFINN_lexicon$score)
test$AFINN.rating = as.vector(term.freq %*% AFINN_lexicon$score)
test
test[c("text","AFINN.rating")]
test[c("text","AFINN.rating", "polarity")]
test$AFINN.rating = as.vector(term.freq %*% AFINN_lexicon$score)
test$pred = sign(test$AFINN.rating)
table(test$pred)
table(test$pred, test$polarity)
table(test$polarity, test$pred)
(102+142)/(102+17+50+142)
length(grep("\\]:", x$text, value = TRUE)) #70 ]:'s in the whole set.
length(grep(":\\{", x$text, value = TRUE)) #32 }:'s in the whole set.
length(grep(":\\{", x$text, value = TRUE)) #32 :{'s in the whole set.
length(grep(":\\{", x$text, value = TRUE)) #32 }:'s in the whole set.
length(grep("\\}:", x$text, value = TRUE)) #32 }:'s in the whole set.
word.freq <- function(document.vector, sparsity = .99){
# construct corpus
temp.corpus <- Corpus(VectorSource(document.vector))
# construct tf matrix and remove sparse terms
temp.tf <- DocumentTermMatrix(temp.corpus,
control = list(stopwords = stopwords('english'),
removeNumbers = T))
temp.tf <- removeSparseTerms(temp.tf, sparsity)
temp.tf <- as.matrix(temp.tf)
# construct word frequency df
freq.df <- colSums(temp.tf)
freq.df <- data.frame(word = names(freq.df), freq = freq.df)
rownames(freq.df) <- NULL
return(freq.df)
}
fruit.names=c("apple","banana","cantaloupe","date")
str_count("i want to eat an apple or a banana, preferably an apple",fruit.names)
str_count("i want to eat an apples or a banana, preferably an apple",fruit.names)
str_count("i want to eat an applesauce or a banana, preferably an apple",fruit.names)
str_count("i want to eat an applesauces or a banana, preferably an apple",fruit.names)
fruit.names=c("app","banana","cantaloupe","date")
str_count("i want to eat an applesauces or a banana, preferably an apple",fruit.names)
fruit.names=c("happy","banana","cantaloupe","date")
str_count("i want to eat an #ohhappyday or a banana, preferably an apple",fruit.names)
rm(x)
