rm(train)
rm(test)
rm(semisuper)
rm(freq.all)
rm(word.freq.neg)
rm(word.freq.pos)
happy_tweets = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", nrows = 150000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
dim(happy_tweets)
sad_tweets = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", nrows = 55000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
dim(sad_tweets)
happy_tweets$clean = clean.data(happy_tweets$text)
sad_tweets$clean = clean.data(sad_tweets$text)
happy_corpus = Corpus(VectorSource(happy_tweets$clean))
sad_corpus = Corpus(VectorSource(sad_tweets$clean))
happy_corpus <- tm_map(happy_corpus, function(x)removeWords(x,stopwords())) # ~2 minutes
sad_corpus <- tm_map(sad_corpus, function(x)removeWords(x,stopwords())) # ~1 minute
#Make word cloud. Check reference for options
wordcloud(happy_corpus, max.words = 200) # ~4 minutes
wordcloud(sad_corpus, max.words = 200) # ~3 minutes
library(wordcloud)
library(tm)
x = read.csv(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.csv", nrows = 940, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
head(x)
wordcloud(happy_corpus, max.words = 200) # ~4 minutes
wordcloud(sad_corpus, max.words = 200) # ~3 minutes
wordcloud(happy_corpus, max.words = 200) # ~4 minutes
test = read.csv("testdata.manual.2009.06.14.csv", header = FALSE, colClasses =
c("character", "character", "character", "character", "character", "character"))
colnames(test) = c("polarity", "not_sure", "created_at", "search_query", "username", "text")
test[test$polarity == 0,]$polarity = 0
test[test$polarity == 4,]$polarity = 1
table(test$polarity)
dim(test)
head(test)
head(test[c(text, polarity)])
head(test[c("text", "polarity"),])
head(test)
test[c("text", "polarity"),]
test
head(test)
table(test$polarity)
head(test,20)
load("my_oauth.Rdata")
happy = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", nrows = 110000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
sad = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", nrows = 50000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
head(happy)
head(happy, 25)
head(happy$text, 25)
head(happy$text, 500)
head(happy$text, 1000)
head(happy$text, 10000)
head(sad$text, 10000)
blurbs = c("hello", "world", "How are you?")
grep("hello", blurbs)
grep("world", blurbs)
grep(c("hello","world"), blurbs)
grep(hello|world", blurbs)
)
grep("hello|world", blurbs)
x = read.csv(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.csv", nrows = 100000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
grep("\\:\\)", x$text, value = TRUE) #67189 :)'s in the whole set. Takes about 11 sec. to run
grep( "\\:\\) | \\(\\: | \\:-\\) | \\(-\\:", x$text, value = TRUE)
grep("\\:\\)", x$text, value = TRUE) #67189 :)'s in the whole set. Takes about 11 sec. to run
grep("\\(\\:", x$text, value = TRUE) #14401 (:'s in the whole set.
grep( "\\:\\)|\\(\\:|\\:-\\)|\\(-\\:", x$text, value = TRUE)
grep( "\\:\\)|\\(\\:|\\:-\\)|\\(-\\:|\\:D|\\:-D|=\\)|\\(=", x$text, value = TRUE)
grep( "\\:\\)|\\(\\:|\\:-\\)|\\(-\\:|\\:D|\\:-D|=\\)|\\(=|☺|☻", x$text, value = TRUE)
x = read.csv(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.csv", nrows = 9400000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
length(grep("\\:\\)|\\(\\:|\\:-\\)|\\(-\\:|\\:D|\\:-D|=\\)|\\(=|☺|☻", x$text, value = FALSE))
happy_indices = c(
grep("\\:\\)", x$text, value = FALSE),
grep("\\(\\:", x$text, value = FALSE),
grep("\\:-\\)", x$text, value = FALSE),
grep("\\(-\\:", x$text, value = FALSE),
grep("\\:D", x$text, value = FALSE),
grep("\\:-D", x$text, value = FALSE),
grep("=\\)", x$text, value = FALSE),
grep("\\(=", x$text, value = FALSE),
grep("☺", x$text, value = FALSE),
grep("☻", x$text, value = TRUE)
)
length(happy_indices)
length(happy_indices) #happy_indices has length 148000
dim(x[happy_indices[duplicated(happy_indices)],]) #698 entries have multiple distinct happy emoticons
dim(unique(x[happy_indices,])) #There are 147176 unique happy rows
happy_indices2 = length(grep("\\:\\)|\\(\\:|\\:-\\)|\\(-\\:|\\:D|\\:-D|=\\)|\\(=|☺|☻", x$text, value = FALSE))
happy_indices2
happy_indices2 = grep("\\:\\)|\\(\\:|\\:-\\)|\\(-\\:|\\:D|\\:-D|=\\)|\\(=|☺|☻", x$text, value = FALSE)
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
paste(happy_emoticons, sep = "|")
help("paste")
happy_emoticons = list("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
paste(happy_emoticons, sep = "|")
pate(1:10)
paste(1:10)
1:10
paste(1:10, sep = "")
paste(1:10, sep = "   ")
paste(1:10, sep = "|", collapse = "")
paste(happy_emoticons, sep = "|", collapse = "")
paste(happy_emoticons, collapse = "|")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
paste(sad_emoticons, collapse = "|")
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
length(happy_indices2)
sad_indices2 = grep(paste(sad_emoticons, collapse = "|"),,x$text, value = FALSE)
sad_indices2 = grep(paste(sad_emoticons, collapse = "|"),x$text, value = FALSE)
length(ssad_indices2)
length(sad_indices2)
52498-52247
head(sad_indices)
head(happy_indices)
head(happy_indices2)
help(match)
happy_indices %in% happy_indices2
table(happy_indices %in% happy_indices2)
table(happy_indices2 %in% happy_indices)
table(happy_indices2 %in% happy_indices, happy_indices %in% happy_indices2)
index(happy_indices2 %in% happy_indices)
x[happy_indices2 %in% happy_indices,]
help(load)
x = load(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.Rdata")
x = load(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.RData")
# HAPPY EMOTICONS ----
x = load(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.RData")
load(file = "~/Desktop/Huang Research/Rsentiment/ComTweetsLA.RData")
load("~/Desktop/Huang Research/Rsentiment/ComTweetsLA.RData")
load("~/Desktop/Huang Research/Rsentiment/comTweetsLA.RData")
AFINN = read.delim(file = "~/Desktop/Documents/GitRepos/Rsentiment/Lexicons/AFINN/AFINN-111.txt", stringsAsFactors = FALSE)
colnames(AFINN) = c("word", "score")
#looking up AFINN words
AFINN[AFINN$score == -5,]
AFINN[pmatch("am", AFINN$word),2]
AFINN[pmatch("feeling", AFINN$word),2]
AFINN[pmatch("creaking", AFINN$word),2]
AFINN[pmatch("plague", AFINN$word),2]
OpinionFinder = read.csv(system.file("data/subjectivity.csv.gz",
package = "sentiment"), header = FALSE, stringsAsFactors = FALSE)
#write.csv(OpinionFinder, file = "OpinionFinder")
OpinionFinder = as.data.frame(cbind(as.character(OpinionFinder$V1), as.integer(2*(OpinionFinder$V3 == "positive")-1)))
colnames(OpinionFinder) = c("word", "score")
OpinionFinder$score = as.integer(OpinionFinder$score)
OpinionFinder$score = (((OpinionFinder$score-1)*2)-1)*-1
#looking up Wiebe words
OpinionFinder[OpinionFinder$score == -1,]
OpinionFinder[pmatch("asu", OpinionFinder$word),] #the word "asu" matches "asunder" and gets a score of -1!!!
OpinionFinder[pmatch("you", OpinionFinder$word),]
NRC = read.csv(file = "~/Desktop/Documents/GitRepos/Rsentiment/Lexicons/EmoLex/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt",
sep = "\t", header = FALSE)
colnames(NRC) = c("word", "emotion", "indicator")
NRC = NRC[NRC$emotion == "negative"|NRC$emotion == "positive",]
NRC = NRC[NRC$indicator == 1,]
NRC[NRC$emotion == "negative",]$indicator = -1
NRC = NRC[c("word", "indicator")]
colnames(NRC) = c("word", "score")
ANEW = read.csv(file = "~/Desktop/Documents/GitRepos/Rsentiment/Lexicons/ANEW.csv", header = FALSE)
colnames(ANEW) = c("word", "score")
#ANEW$score = ANEW$score-(mean(ANEW$score)+1) #normalize ANEW scores to 0. This didn't work as well as the next line did.
ANEW$score = ANEW$score - 5
range(ANEW$score)
head(ANEW[order(-ANEW$score),], 20) #happiest words
head(ANEW[order(ANEW$score),], 20) #saddest words
ls()
rm(ls())
rm(as.list(ls))
as.list(ls)
list(ls())
rm(list(ls))
rm(x)
rm(AFINN)
rm(AFINN_lexicon)
rm(ANEW)
rm(happy)
rm(happy_tweets)
rm(NRC)
rm(OpinionFinder)
rm(sad)
rm(sad_tweets)
rm(test)
rm(Wiebe_lexicon)
rm(blurbs)
rm(f1)
rm(happy_corpus)
rm(happy_emoticons)
rm(happy_indices)
rm(happy_indices2)
RM(index)
rm(iindex)
rm(index)
rm(inv.doc.freq)
rm(precip)
rm(precision)
rm(recall)
rm(sad_corpus)
rm(sad_emoticons)
rm(sad_indices2)
rm(AFINN_lexicon.frequencies())
rm(AFINN_lexicon.frequencies
)
rm(clean.data)
rm(ndsi.frequencies)
rm(word.freq)
library(stringr) #library for str_count function
library(e1071) # for naive bayes model
library(ggplot2) #for graphs
library(caret) #for confusionMatrix
library(pROC) #ROC curves
library(randomForest)
library(tm) # for building term frequency matrix from corpus
source("functions.R") #get cleaning function, AFINN_lexicon
load("~/Desktop/Huang Research/Rsentiment/comTweetsLA.RData") # load LA2014 into memory as x
happy_indices = grep(paste(happy_emoticons, collapse = "|"),x$text, value = FALSE)
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
happy_indices = grep(paste(happy_emoticons, collapse = "|"),x$text, value = FALSE)
length(happy_indices) #happy_indices has length 148000
dim(x[happy_indices[duplicated(happy_indices)],]) #698 entries have multiple distinct happy emoticons
dim(unique(x[happy_indices,])) #There are 147176 unique happy rows
x = x[happy_indices,] #Rewrite over x to avoid memory problems
dim(x) # Looks good
write.csv(x,file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", row.names = FALSE)
rm(x)
load("~/Desktop/Huang Research/Rsentiment/comTweetsLA.RData") # load LA2014 into memory as x
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
sad_indices = grep(paste(sad_emoticons, collapse = "|"),x$text, value = FALSE)
length(sad_indices) #sad_indices has length
x = unique(x[sad_indices,]) #Write over x to avoid memory problems
dim(x) # Looks good
write.csv(x,file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", row.names = FALSE)
rm(x)
library(wordcloud)
library(tm)
# load happy and sad tweets
happy_tweets = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", nrows = 150000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
dim(happy_tweets)
sad_tweets = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", nrows = 55000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
dim(sad_tweets)
#clean tweets
# Note that tokenization was turned off for this step to avoid getting a hunge "USERNAME" in the middle of the
happy_tweets$clean = clean.data(happy_tweets$text)
sad_tweets$clean = clean.data(sad_tweets$text)
import("functions.R")
source("functions.R")
happy_tweets$clean = clean.data(happy_tweets$text)
sad_tweets$clean = clean.data(sad_tweets$text)
#Create corpus using tm package
happy_corpus = Corpus(VectorSource(happy_tweets$clean))
sad_corpus = Corpus(VectorSource(sad_tweets$clean))
happy_tweets$clean = clean.data(happy_tweets$text)
happy_tweets$clean = clean.tweets(happy_tweets$text)
sad_tweets$clean = clean.tweets(sad_tweets$text)
#Create corpus using tm package
happy_corpus = Corpus(VectorSource(happy_tweets$clean))
sad_corpus = Corpus(VectorSource(sad_tweets$clean))
happy_corpus <- tm_map(happy_corpus, function(x)removeWords(x,stopwords())) # ~2 minutes
sad_corpus <- tm_map(sad_corpus, function(x)removeWords(x,stopwords())) # ~1 minute
2+2
2+2
cleantext = laply(documents, function(documents)
{
documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("RT", "retweet", documents) # tokenize retweets
documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
if(keepHashText == TRUE){documents = gsub("#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.}
else{documents = gsub("#\\w+", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.}
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
clean.tweets = function(documents, usernameToken = "username", urlToken = "url", hashToken = "hash ", keepHashText = TRUE){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("RT", "retweet", documents) # tokenize retweets
documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
if(keepHashText == TRUE){
documents = gsub("#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
}
else{
documents = gsub("#\\w+", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
}
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
library(wordcloud)
library(tm)
source("functions.R")
library(wordcloud)
library(tm)
# load happy and sad tweets
happy_tweets = read.csv(file = "~/Desktop/Huang Research/Rsentiment/happy_tweets_2014", nrows = 150000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
dim(happy_tweets)
sad_tweets = read.csv(file = "~/Desktop/Huang Research/Rsentiment/sad_tweets_2014", nrows = 55000, header = TRUE, colClasses =
c("character", "character", "character", "numeric", "numeric", "integer", "integer", "integer", "integer", "integer", "integer"))
dim(sad_tweets)
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "", keepHashText = TRUE)
sad_tweets$clean = clean.tweets(sad_tweets$text, usernameToken = "", hashToken = "", keepHashText = TRUE)
clean.tweets = function(documents, usernameToken = "username", urlToken = "url", hashToken = "hash ", keepHashText = TRUE){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
documents = gsub("RT", "retweet", documents) # tokenize retweets
documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
if(keepHashText == TRUE){
documents = gsub("#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
}
else{
documents = gsub("#\\w+", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
}
documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "", keepHashText = TRUE)
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "", keepHashText = TRUE)
if(1==1){}
if(1==1){print("hello")}else{print("world")}
if(1!=1){print("hello")}else{print("world")}
gsub("#", "","#so happy #happy happy")
gsub("#\\w+", "","#so happy #happy happy")
gsub("#", "","#so happy #happy happy")
clean.tweets = function(documents, usernameToken = "username", urlToken = "url", hashToken = "hash ", keepHashText = TRUE){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
documents = gsub("RT", "retweet", documents) # tokenize retweets
documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
#if(keepHashText == TRUE){
#  documents = gsub("\\#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
#  }
#else{
#  documents = gsub("#\\w+", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
#}
documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
sad_tweets$clean = clean.tweets(sad_tweets$text, usernameToken = "", hashToken = "", keepHashText = TRUE)
clean.tweets = function(documents, usernameToken = "username", urlToken = "url", hashToken = "hash "){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
documents = gsub("RT", "retweet", documents) # tokenize retweets
documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
documents = gsub("\\#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
sad_tweets$clean = clean.tweets(sad_tweets$text, usernameToken = "", hashToken = "")
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "")
happy_tweets$text
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "")
clean.tweets = function(documents, usernameToken = "username", urlToken = "url", hashToken = "hash "){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
documents = gsub("RT", "retweet", documents) # tokenize retweets
#documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
documents = gsub("\\#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
#documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
#documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "")
source("functions.R")
happy_corpus = Corpus(VectorSource(happy_tweets$clean))
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "")
clean.tweets = function(documents, usernameToken = "username", urlToken = "url", hashToken = "hash "){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
documents = gsub("RT", "retweet", documents) # tokenize retweets
#documents = rm_url(documents, replacement = urlToken) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
documents = gsub("\\#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
documents = gsub(paste(sad_emoticons, collapse = "|"), "sademoticon", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), "happyemoticon", documents) #tokenize happy emoticons
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("http\\w+", "", documents) #remove hyperlinks
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "")
happy_tweets$text[1:1000]
happy_tweets$clean[1:1000]
clean.tweets = function(documents, usernameToken = "username", hashToken = "hash "){
happy_emoticons = c("\\:\\)" , "\\(\\:", "\\:-\\)", "\\(-\\:", "\\:D", "\\:-D", "=\\)", "\\(=", "☺", "☻")
sad_emoticons = c("\\:\\(", "\\:-\\(", "\\)\\:", "\\)-\\:", ":\\[", "\\]:", ":\\{", "\\}:","=\\(", "\\)=", "☹")
require(plyr)
require(dplyr)
require(qdapRegex)
cleantext = laply(documents, function(documents)
{
#documents = gsub("RT", "retweet", documents) # tokenize retweets. Ignore this since tweets aren't retweeets
documents = rm_url(documents) #tokenize urls
documents = gsub("@\\w+", usernameToken, documents) #tokenize @
documents = gsub("\\#", hashToken, documents) #tokenize #. Not necessary for tweets that haven't been classified yet.
documents = gsub(paste(sad_emoticons, collapse = "|"), " sademoticon ", documents) #tokenize sad emoticons
documents = gsub(paste(happy_emoticons, collapse = "|"), " happyemoticon ", documents) #tokenize happy emoticons
documents = gsub("[[:punct:]]", "", documents) #remove punctuation
documents = gsub("[[:digit:]]", "", documents) #remove numbers
documents = gsub("[^a-zA-Z]", " ", documents) #remove everything that isn't a letter
documents = tolower(documents) #set lower case
documents<-gsub('([[:alpha:]])\\1+', '\\1\\1', documents) # limit character repeats to maximum 2
documents<-trimws(documents) #remove leading and trailing whitespace
}, .progress = "text")
return(cleantext)
}
sad_tweets$clean = clean.tweets(sad_tweets$text, usernameToken = "", hashToken = "")
happy_tweets$clean = clean.tweets(happy_tweets$text, usernameToken = "", hashToken = "")
#Create corpus using tm package
happy_corpus = Corpus(VectorSource(happy_tweets$clean))
sad_corpus = Corpus(VectorSource(sad_tweets$clean))
#Remove stop words
happy_corpus <- tm_map(happy_corpus, function(x)removeWords(x,stopwords())) # ~2 minutes
sad_corpus <- tm_map(sad_corpus, function(x)removeWords(x,stopwords())) # ~1 minute
#Make word cloud. Check reference for options
wordcloud(happy_corpus, max.words = 200) # ~4 minutes
wordcloud(sad_corpus, max.words = 200) # ~3 minutes
